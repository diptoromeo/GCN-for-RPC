{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# import pandas as pd\n",
        "\n",
        "# # Define the standard deviation data\n",
        "# data = {\n",
        "#     \"arXiv_Acc\": [1.04, 0.93, 0.81, 0.79, 0.84, 0.86, 0.80, 0.78, 0.61, 0.38],\n",
        "#     \"DBLP_Acc\": [1.15, 1.13, 0.96, 1.10, 0.88, 0.64, 0.77, 0.72, 0.55, 0.40],\n",
        "#     \"Elsevier_Acc\": [1.00, 1.06, 0.71, 1.09, 0.87, 0.96, 0.69, 0.91, 0.83, 0.34],\n",
        "#     \"PubMed_Acc\": [1.27, 1.36, 0.99, 1.00, 0.77, 0.66, 0.68, 0.63, 0.59, 0.36],\n",
        "# }\n",
        "\n",
        "# # Convert to DataFrame for easier plotting\n",
        "# df = pd.DataFrame(data)\n",
        "\n",
        "# # Define model names\n",
        "# model_names = [\n",
        "#     \"ManiReg\", \"SemiEmb\", \"LP\", \"DeepWalk\", \"ICA\",\n",
        "#     \"Planetoid\", \"Chebyshev\", \"GCNs\", \"MoNet\", \"GRCN-BERT\"\n",
        "# ]\n",
        "\n",
        "# # Create the boxplot\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# plt.boxplot(df.values, labels=df.columns)\n",
        "\n",
        "# # Add model name labels manually for clarity\n",
        "# for i, dataset in enumerate(df.columns):\n",
        "#     for j, value in enumerate(df[dataset]):\n",
        "#         plt.text(i + 1.1, value, model_names[j], fontsize=8, verticalalignment='center')\n",
        "\n",
        "# plt.title(\"Standard Deviation of Accuracy by Dataset and Model\")\n",
        "# plt.ylabel(\"Standard Deviation\")\n",
        "# plt.xticks(rotation=45)\n",
        "# plt.ylim(0, 2)\n",
        "# plt.grid(True)\n",
        "# plt.tight_layout()\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "id": "9jxBmE3Zn8H1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#datasets = ['Arxiv_AI','Arxiv_metadata','Elsevier','IEEE']"
      ],
      "metadata": {
        "id": "3sm19Bq2DxZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkH1Kly9Mi6c",
        "outputId": "eeaf5eef-2f55-48a4-94bb-69c2384c0a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32072\n",
            "The Average length of abstract in list is : 1417.3098341232228\n",
            "32072\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "#df = pd.read_csv(\"/content/arXiv_2023.csv\")\n",
        "#df.head()\n",
        "df = pd.read_json(\"/content/elsevier.json\", orient='records', lines=True)\n",
        "df.head()\n",
        "\n",
        "all_input = []\n",
        "if 'abstract' in df.columns:\n",
        "    all_input = df['abstract'].tolist()\n",
        "\n",
        "print(len(all_input))\n",
        "\n",
        "# Filter out non-string elements from the list\n",
        "all_input = [x for x in all_input if isinstance(x, str)]\n",
        "\n",
        "# Calculate average length for valid strings\n",
        "res = sum(map(len, all_input))/float(len(all_input)) if all_input else 0 # Handle empty list\n",
        "\n",
        "\n",
        "print(\"The Average length of abstract in list is : \" + str(res))\n",
        "print(len(all_input))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h8Bl8oRFvHaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "310816c8-f921-49cd-fd62-5696faca052c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ],
      "source": [
        "# all_input = all_input[:10000]\n",
        "# print(len(all_input))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JPT1ZS1ZpDN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "arxiv_meta_words = ['system', 'field', 'energy', 'structure', 'quantum']\n",
        "#, 'network', 'information','phase', 'distribution','density',\n",
        " #                   'temperature', 'framework', 'graph','transition', 'power']#, 'image', 'emission', 'surface', 'interaction', 'value',\n",
        "                    # 'detection', 'algorithm','formation', 'source','control', 'region', 'gas', 'spin', 'star', 'symmetry', 'design',\n",
        "                    # 'spectrum','velocity', 'measure','frequency', 'mechanism', 'particle', 'correlation', 'classification', 'motion',\n",
        "                    # 'simulation', 'optimization', 'distance', 'application', 'technique','dependence', 'relation']\n",
        "\n",
        "\n",
        "arxiv_ai_words = ['knowledge', 'system', 'information', 'framework', 'decision']#,'search', 'network','language', 'algorithm',\n",
        "                  #'intelligence', 'work', 'graph', 'plan', 'inference', 'space']#, 'state', 'theory', 'game', 'machine', 'agent',\n",
        "                  # 'function','structure','domain', 'ai','solution', 'learn', 'environment', 'optimization','reinforcement',\n",
        "                  # 'complexity','development', 'goal','belief','world']\n",
        "\n",
        "\n",
        "elsevier_words = ['energy', 'system', 'development', 'water', 'cell']#, 'health', 'treatment', 'information', 'production',\n",
        "                  #'expression', 'response', 'age', 'disease', 'evidence', 'growth'] #, 'surface', 'temperature', 'brain',\n",
        "                  # 'gene', 'protein', 'structure', 'function', 'management', 'design', 'area', 'case', 'quality','work', 'power',\n",
        "                  # 'cancer', 'food', 'field']\n",
        "\n",
        "ieee = ['network', 'power', 'information', 'image', 'analysis']#, 'detection',  'time', 'algorithm', 'energy', 'optimization',\n",
        "        #'classification', 'security', 'work', 'machine', 'simulation', 'quality']\n",
        "\n",
        "# from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "# doc_labels = []\n",
        "\n",
        "# for doc in all_input:\n",
        "#     label = []\n",
        "#     for term in ieee:\n",
        "#         if term in doc:\n",
        "#             label.append(1)\n",
        "#         else:\n",
        "#             label.append(0)\n",
        "#     doc_labels.append(label)\n",
        "\n",
        "# print(\"doc_labels:\", len(doc_labels))\n",
        "# print(doc_labels[:5])\n",
        "\n",
        "\n",
        "# train_labels, test_labels = train_test_split(doc_labels, test_size=0.2, random_state=42)\n",
        "# print(len(train_labels))\n",
        "# print(len(test_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5G_RPx1RecS",
        "outputId": "50332f5e-cc79-406d-e6a5-d78ea0f67477"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "doc_labels: 10000\n",
            "[[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1], [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0]]\n",
            "8000\n",
            "2000\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "doc_labels = []\n",
        "\n",
        "for doc in all_input:\n",
        "    label = []\n",
        "    for term in elsevier_words:\n",
        "        if term in doc:\n",
        "            label.append(1)\n",
        "        else:\n",
        "            label.append(0)\n",
        "    doc_labels.append(label)\n",
        "\n",
        "print(\"doc_labels:\", len(doc_labels))\n",
        "print(doc_labels[:5])\n",
        "\n",
        "\n",
        "train_labels, test_labels = train_test_split(doc_labels, test_size=0.2, random_state=42)\n",
        "print(len(train_labels))\n",
        "print(len(test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yw5ms6DJFxGI"
      },
      "source": [
        "# Train and Test Split Randomly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaeNRrx_zNCd",
        "outputId": "45b58091-ca2b-4a5e-fa5a-ddf3d0617b6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000\n",
            "2000\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "random.shuffle(all_input)\n",
        "#random.shuffle(all_keywords)\n",
        "\n",
        "\n",
        "train_size = int(0.8 * len(all_input))\n",
        "train_input = all_input[:train_size]\n",
        "test_input = all_input[train_size:]\n",
        "\n",
        "print(len(train_input))\n",
        "print(len(test_input))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L02OWKOQ_lQ"
      },
      "source": [
        "# Single data label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYCVb120U16V"
      },
      "outputs": [],
      "source": [
        "# train_labels = []\n",
        "\n",
        "# for doc in train_input:\n",
        "#     label = []\n",
        "#     for term in ieee:\n",
        "#         if term in doc:\n",
        "#             label.append(1)\n",
        "#         else:\n",
        "#             label.append(0)\n",
        "#     train_labels.append(label)\n",
        "\n",
        "# print(\"train_labels:\", len(train_labels))\n",
        "\n",
        "# test_labels = []\n",
        "\n",
        "# for doc in test_input:\n",
        "#     label = []\n",
        "#     for term in ieee:\n",
        "#         if term in doc:\n",
        "#             label.append(1)\n",
        "#         else:\n",
        "#             label.append(0)\n",
        "#     test_labels.append(label)\n",
        "\n",
        "# print(\"test_labels:\", len(test_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJPI-IXrBkrP"
      },
      "source": [
        "# Dataset Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GyzNkI7W03D",
        "outputId": "0e93f044-58b6-40b4-877f-f4466866291b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8000\n",
            "2000\n",
            "10000\n"
          ]
        }
      ],
      "source": [
        "original_train_sentences = train_input\n",
        "original_labels_train = train_labels\n",
        "original_test_sentences = test_input\n",
        "original_labels_test = test_labels\n",
        "\n",
        "# example\n",
        "# original_train_sentences = ['this is sample 1','this is sample 2']\n",
        "# original_labels_train = ['postive','negative']\n",
        "# original_test_sentences = ['this is sample 1','this is sample 2']\n",
        "# original_labels_test = ['postive','negative']\n",
        "\n",
        "train_size = len(original_train_sentences)\n",
        "test_size = len(original_test_sentences)\n",
        "sentences = original_train_sentences + original_test_sentences\n",
        "\n",
        "print(train_size)\n",
        "print(test_size)\n",
        "print(len(sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6K9dWTv5I07_"
      },
      "source": [
        "# Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdtNfM6pxwoM"
      },
      "outputs": [],
      "source": [
        "Sg = 0 #0:CBOW 1:SG\n",
        "EDGE = 1 # 0:d2w 1:d2w+w2w 2:d2w+w2w+d2d\n",
        "NODE = 0 # 0:one-hot #1:BERT\n",
        "NUM_LAYERS = 2\n",
        "graph_window_sizes = 20\n",
        "HIDDEN_DIM = 200\n",
        "DROP_OUT = 0.3  #DROP_OUT = 0.5\n",
        "LR = 0.01  #LR = 0.01 for cs data\n",
        "WEIGHT_DECAY = 0\n",
        "EARLY_STOPPING = 145  # EARLY_STOPPING = 10\n",
        "NUM_EPOCHS = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2W7wKTBfa71"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hobYcJ5OX5oT"
      },
      "source": [
        "[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7zWIYZexK1E8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "\n",
        "# Assuming original_labels_train is a list of lists, where each inner list contains the labels for a single sample\n",
        "mlb = MultiLabelBinarizer()\n",
        "train_labels = mlb.fit_transform(original_labels_train)\n",
        "test_labels = mlb.transform(original_labels_test)\n",
        "\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming you want to flatten the labels into a single list\n",
        "labels = train_labels.flatten().tolist() + test_labels.flatten().tolist()  #CS_test=1172  #SS_test =1023\n",
        "labels = torch.LongTensor(labels).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uaFw4OP1GHcu"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# # Combine labels from both train and test sets to ensure all labels are seen\n",
        "# all_labels = original_labels_train + original_labels_test\n",
        "# unique_labels = np.unique(all_labels)\n",
        "\n",
        "# num_class = len(unique_labels)\n",
        "# lEnc = LabelEncoder()\n",
        "# lEnc.fit(unique_labels)  # Fit on all unique labels\n",
        "\n",
        "# train_labels = lEnc.transform(original_labels_train)\n",
        "# test_labels = lEnc.transform(original_labels_test)  # Now all labels should be known\n",
        "\n",
        "# import torch\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# labels = train_labels.tolist() + test_labels.tolist()\n",
        "# labels = torch.LongTensor(labels).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJFyRAwbMZ8T"
      },
      "source": [
        "# 2차원 라베일 데이터"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq16RQ8sXPWa"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# unique_labels=np.unique(np.concatenate(original_labels_train)) # Flatten the list to get unique labels\n",
        "\n",
        "# num_class = len(unique_labels)\n",
        "# lEnc = LabelEncoder()\n",
        "# lEnc.fit(unique_labels)\n",
        "\n",
        "# # print(unique_labels)\n",
        "# # print(lEnc.transform(unique_labels))\n",
        "\n",
        "# # Reshape the labels to be 1D\n",
        "# train_labels = np.array([lEnc.transform(sample) for sample in original_labels_train]).flatten()\n",
        "# test_labels = np.array([lEnc.transform(sample) for sample in original_labels_test]).flatten()\n",
        "\n",
        "# import torch\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# labels = train_labels.tolist()+test_labels.tolist()\n",
        "# labels = torch.LongTensor(labels).to(device)\n",
        "\n",
        "# print(\"labels:\",len(labels))\n",
        "\n",
        "# labels = test_labels.tolist()\n",
        "# test_labels = torch.LongTensor(test_labels[:111]).to(device) #meta= 6000  #ai = 2032 # elsevier = 6415 # ieee = 111\n",
        "\n",
        "# print(\"labels:\",len(test_labels))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMkEBxr6fMQi"
      },
      "source": [
        "## Remove Stopwords and less frequent words, tokenize sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1xRG94uDfaBV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80cbaa05-62ad-4373-ef23-e1aa20bd77b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_length: 20392\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import nltk\n",
        "import re\n",
        "\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lm = WordNetLemmatizer()\n",
        "#singular = singularize()\n",
        "\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "remove_limit = 5\n",
        "\n",
        "\n",
        "def clean_str(string):\n",
        "    string = re.sub(r\"[^A-Za-z(),!?\\'\\`]\", \" \", string)\n",
        "    string = re.sub(r'[0-9]+', '', string)\n",
        "    string = re.sub(r'[^\\w\\d\\s]+', '', string)\n",
        "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
        "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
        "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
        "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
        "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
        "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
        "    string = re.sub(r\",\", \" , \", string)\n",
        "    string = re.sub(r\"!\", \" ! \", string)\n",
        "    string = re.sub(r\"\\(\", \" \", string)\n",
        "    string = re.sub(r\"\\)\", \" \", string)\n",
        "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
        "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
        "    string = re.sub(r\",\", \"\", string)\n",
        "    #string = singular.singularize(string)\n",
        "    #string = lm.lemmatize(string, pos='v')\n",
        "    return string.strip().lower()\n",
        "\n",
        "original_word_freq = {}  # to remove rare words\n",
        "for sentence in sentences:\n",
        "    temp = clean_str(sentence)\n",
        "    word_list = temp.split()\n",
        "    for word in word_list:\n",
        "        if word in original_word_freq:\n",
        "            original_word_freq[word] += 1\n",
        "        else:\n",
        "            original_word_freq[word] = 1\n",
        "\n",
        "tokenize_sentences = []\n",
        "word_list_dict = {}\n",
        "for sentence in sentences:\n",
        "    temp = clean_str(sentence)\n",
        "    word_list_temp = temp.split()\n",
        "    doc_words = []\n",
        "    for word in word_list_temp:\n",
        "        if word in original_word_freq and word not in stop_words and original_word_freq[word] >= remove_limit:\n",
        "            doc_words.append(word)\n",
        "            word_list_dict[word] = 1\n",
        "    tokenize_sentences.append(doc_words)\n",
        "word_list = list(word_list_dict.keys())\n",
        "vocab_length = len(word_list)\n",
        "print(\"vocab_length:\", vocab_length)\n",
        "\n",
        "#word to id dict\n",
        "word_id_map = {}\n",
        "for i in range(vocab_length):\n",
        "    word_id_map[word_list[i]] = i\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqLUncB2Pn_L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa66422a-3585-4ae2-a50d-69c911645ea2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "node_size: 30392\n"
          ]
        }
      ],
      "source": [
        "node_size = train_size + vocab_length + test_size\n",
        "print('node_size:', node_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0o8wcXgrTiD"
      },
      "source": [
        "# Model input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZbRV2wYxY1U"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znJ7Grz7fQ2L"
      },
      "source": [
        "## Build Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BSg1uNgV3_7"
      },
      "outputs": [],
      "source": [
        "from math import log\n",
        "row = []\n",
        "col = []\n",
        "weight = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QESQPT88AqsI"
      },
      "source": [
        "### word-word: PMI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "7aa5370929264d4d9b49873dc790c2de",
            "31b6ba592bfe4ff3bc0d93ed57e66909",
            "4fe1ba2fb3b743e49d1bdc21da4a0b49",
            "145c11efe2b24cacb9d630b782561fd4",
            "a87e348731394ef884abf4051437cb68",
            "932edc72a8ea416994ee5fb6d58f0132",
            "a3a14471e6334865b2c4ca821d19652c",
            "292da0fcd01c4fadb8758924716c50db",
            "b43c97188d054b4e95ac1bd871eff696",
            "5981e8d3acc248cd8627e6146ce985ae",
            "8f4254b16af344229ff79b4716896996"
          ]
        },
        "id": "KNlJoLFagXhv",
        "outputId": "6e1aec3a-53fb-434b-c09d-ead55b40e3ed"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7aa5370929264d4d9b49873dc790c2de"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "window_size = graph_window_sizes\n",
        "total_W = 0\n",
        "word_occurrence = {}\n",
        "word_pair_occurrence = {}\n",
        "\n",
        "def ordered_word_pair(a, b):\n",
        "    if a > b:\n",
        "        return b, a\n",
        "    else:\n",
        "        return a, b\n",
        "\n",
        "def update_word_and_word_pair_occurrence(q):\n",
        "    unique_q = list(set(q))\n",
        "    for i in unique_q:\n",
        "        try:\n",
        "            word_occurrence[i] += 1\n",
        "        except:\n",
        "            word_occurrence[i] = 1\n",
        "    for i in range(len(unique_q)):\n",
        "        for j in range(i+1, len(unique_q)):\n",
        "            word1 = unique_q[i]\n",
        "            word2 = unique_q[j]\n",
        "            word1, word2 = ordered_word_pair(word1, word2)\n",
        "            try:\n",
        "                word_pair_occurrence[(word1, word2)] += 1\n",
        "            except:\n",
        "                word_pair_occurrence[(word1, word2)] = 1\n",
        "\n",
        "\n",
        "for ind in tqdm(range(train_size+test_size)):\n",
        "    words = tokenize_sentences[ind]\n",
        "\n",
        "    q = []\n",
        "    # push the first (window_size) words into a queue\n",
        "    for i in range(min(window_size, len(words))):\n",
        "        q += [word_id_map[words[i]]]\n",
        "    # update the total number of the sliding windows\n",
        "    total_W += 1\n",
        "    # update the number of sliding windows that contain each word and word pair\n",
        "    update_word_and_word_pair_occurrence(q)\n",
        "\n",
        "    now_next_word_index = window_size\n",
        "    # pop the first word out and let the next word in, keep doing this until the end of the document\n",
        "    while now_next_word_index<len(words):\n",
        "        q.pop(0)\n",
        "        q += [word_id_map[words[now_next_word_index]]]\n",
        "        now_next_word_index += 1\n",
        "        # update the total number of the sliding windows\n",
        "        total_W += 1\n",
        "        # update the number of sliding windows that contain each word and word pair\n",
        "        update_word_and_word_pair_occurrence(q)\n",
        "\n",
        "for word_pair in word_pair_occurrence:\n",
        "    i = word_pair[0]\n",
        "    j = word_pair[1]\n",
        "    count = word_pair_occurrence[word_pair]\n",
        "    word_freq_i = word_occurrence[i]\n",
        "    word_freq_j = word_occurrence[j]\n",
        "    pmi = log((count * total_W) / (word_freq_i * word_freq_j))\n",
        "    if pmi <=0:\n",
        "        continue\n",
        "    row.append(train_size + i)\n",
        "    col.append(train_size + j)\n",
        "    weight.append(pmi)\n",
        "    row.append(train_size + j)\n",
        "    col.append(train_size + i)\n",
        "    weight.append(pmi)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## word-word memory and time check"
      ],
      "metadata": {
        "id": "U5XE24_uDjGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: capture memory usage and woring time for word-word\n",
        "\n",
        "%%capture --no-stderr --no-display\n",
        "import time\n",
        "import gc\n",
        "import resource\n",
        "\n",
        "# Clean up memory before starting the measurement\n",
        "gc.collect()\n",
        "# Get current memory usage in bytes\n",
        "mem_before = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Paste the code block that defines and runs word-word calculation here\n",
        "# Starting from the comment # ### word-word: PMI\n",
        "# Make sure to include all necessary variables and imports defined earlier in your script\n",
        "# For example, ensure `tokenize_sentences`, `train_size`, `test_size`, `word_id_map`,\n",
        "# and `graph_window_sizes` are available.\n",
        "\n",
        "# ### word-word: PMI\n",
        "window_size = graph_window_sizes\n",
        "total_W = 0\n",
        "word_occurrence = {}\n",
        "word_pair_occurrence = {}\n",
        "\n",
        "def ordered_word_pair(a, b):\n",
        "    if a > b:\n",
        "        return b, a\n",
        "    else:\n",
        "        return a, b\n",
        "\n",
        "def update_word_and_word_pair_occurrence(q):\n",
        "    unique_q = list(set(q))\n",
        "    for i in unique_q:\n",
        "        try:\n",
        "            word_occurrence[i] += 1\n",
        "        except:\n",
        "            word_occurrence[i] = 1\n",
        "    for i in range(len(unique_q)):\n",
        "        for j in range(i+1, len(unique_q)):\n",
        "            word1 = unique_q[i]\n",
        "            word2 = unique_q[j]\n",
        "            word1, word2 = ordered_word_pair(word1, word2)\n",
        "            try:\n",
        "                word_pair_occurrence[(word1, word2)] += 1\n",
        "            except:\n",
        "                word_pair_occurrence[(word1, word2)] = 1\n",
        "\n",
        "\n",
        "for ind in tqdm(range(train_size+test_size)):\n",
        "    words = tokenize_sentences[ind]\n",
        "\n",
        "    q = []\n",
        "    # push the first (window_size) words into a queue\n",
        "    for i in range(min(window_size, len(words))):\n",
        "        q += [word_id_map[words[i]]]\n",
        "    # update the total number of the sliding windows\n",
        "    total_W += 1\n",
        "    # update the number of sliding windows that contain each word and word pair\n",
        "    update_word_and_word_pair_occurrence(q)\n",
        "\n",
        "    now_next_word_index = window_size\n",
        "    # pop the first word out and let the next word in, keep doing this until the end of the document\n",
        "    while now_next_word_index<len(words):\n",
        "        q.pop(0)\n",
        "        q += [word_id_map[words[now_next_word_index]]]\n",
        "        now_next_word_index += 1\n",
        "        # update the total number of the sliding windows\n",
        "        total_W += 1\n",
        "        # update the number of sliding windows that contain each word and word pair\n",
        "        update_word_and_word_pair_occurrence(q)\n",
        "\n",
        "for word_pair in word_pair_occurrence:\n",
        "    i = word_pair[0]\n",
        "    j = word_pair[1]\n",
        "    count = word_pair_occurrence[word_pair]\n",
        "    word_freq_i = word_occurrence[i]\n",
        "    word_freq_j = word_occurrence[j]\n",
        "    pmi = log((count * total_W) / (word_freq_i * word_freq_j))\n",
        "    if pmi <=0:\n",
        "        continue\n",
        "    row.append(train_size + i)\n",
        "    col.append(train_size + j)\n",
        "    weight.append(pmi)\n",
        "    row.append(train_size + j)\n",
        "    col.append(train_size + i)\n",
        "    weight.append(pmi)\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Clean up memory after the task\n",
        "gc.collect()\n",
        "# Get current memory usage in bytes\n",
        "mem_after = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
        "\n",
        "time_taken = end_time - start_time\n",
        "memory_used = mem_after - mem_before # Difference in max resident set size\n",
        "\n",
        "print(f\"Time taken for word-word: {time_taken:.4f} seconds\")\n",
        "print(f\"Memory used for word-word: {memory_used} bytes\")\n",
        "print(f\"Memory used for word-word (MB): {memory_used / (1024 * 1024):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "8ace831efc0b4e8baaaab32c5d9d5a82",
            "5bab41534674409f97fd08b145d27737",
            "013aab60b32c4e67bee2b1a072ee32a9",
            "7d3ce23fe96742338a9b69170b36e512",
            "6e6e0e1241ed443ebb3d7fd85845e2db",
            "227f23e470b84cd2be605344b9f10aeb",
            "9b95353a0bf147b89ae86d377e027fc9",
            "960cf9d192c44c6c94822a468ee8469a",
            "ce777699e3e24ba79457541d1605c772",
            "c99da0d989174bb5a14c35d7eb9972ad",
            "0ea9f931f704478c97eb0620706c3df0"
          ]
        },
        "id": "UgVRbIRpDCSj",
        "outputId": "aa8633ec-97e0-4d03-9adb-a427ea5739da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ace831efc0b4e8baaaab32c5d9d5a82"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hynLnT3a33kW"
      },
      "source": [
        "### doc-word: Tf-idf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BnSPqhg1lHps"
      },
      "outputs": [],
      "source": [
        "#get each word appears in which document\n",
        "word_doc_list = {}\n",
        "for word in word_list:\n",
        "    word_doc_list[word]=[]\n",
        "\n",
        "for i in range(len(tokenize_sentences)):\n",
        "    doc_words = tokenize_sentences[i]\n",
        "    unique_words = set(doc_words)\n",
        "    for word in unique_words:\n",
        "        exsit_list = word_doc_list[word]\n",
        "        exsit_list.append(i)\n",
        "        word_doc_list[word] = exsit_list\n",
        "\n",
        "#document frequency\n",
        "word_doc_freq = {}\n",
        "for word, doc_list in word_doc_list.items():\n",
        "    word_doc_freq[word] = len(doc_list)\n",
        "\n",
        "# term frequency\n",
        "doc_word_freq = {}\n",
        "\n",
        "for doc_id in range(len(tokenize_sentences)):\n",
        "    words = tokenize_sentences[doc_id]\n",
        "    for word in words:\n",
        "        word_id = word_id_map[word]\n",
        "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
        "        if doc_word_str in doc_word_freq:\n",
        "            doc_word_freq[doc_word_str] += 1\n",
        "        else:\n",
        "            doc_word_freq[doc_word_str] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z6elPPFO_sXp"
      },
      "outputs": [],
      "source": [
        "for i in range(len(tokenize_sentences)):\n",
        "    words = tokenize_sentences[i]\n",
        "    doc_word_set = set()\n",
        "    for word in words:\n",
        "        if word in doc_word_set:\n",
        "            continue\n",
        "        j = word_id_map[word]\n",
        "        key = str(i) + ',' + str(j)\n",
        "        freq = doc_word_freq[key]\n",
        "        if i < train_size:\n",
        "            row.append(i)\n",
        "        else:\n",
        "            row.append(i + vocab_length)\n",
        "        col.append(train_size + j)\n",
        "        idf = log(1.0 * len(tokenize_sentences) / word_doc_freq[word_list[j]])\n",
        "        weight.append(freq * idf)\n",
        "        doc_word_set.add(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## paper-word moemory and time check"
      ],
      "metadata": {
        "id": "jo3TcqN4D-v_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: capture memory usage and working time for paper-word\n",
        "\n",
        "%%capture --no-stderr --no-display\n",
        "\n",
        "# Clean up memory before starting the measurement\n",
        "gc.collect()\n",
        "# Get current memory usage in bytes\n",
        "mem_before = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Paste the code block that defines and runs word-word calculation here\n",
        "# Starting from the comment # ### doc-word: Tf-idf\n",
        "# Make sure to include all necessary variables and imports defined earlier in your script\n",
        "# For example, ensure `tokenize_sentences`, `train_size`, `vocab_length`,\n",
        "# and `word_id_map` are available.\n",
        "\n",
        "# ### doc-word: Tf-idf\n",
        "#get each word appears in which document\n",
        "word_doc_list = {}\n",
        "for word in word_list:\n",
        "    word_doc_list[word]=[]\n",
        "\n",
        "for i in range(len(tokenize_sentences)):\n",
        "    doc_words = tokenize_sentences[i]\n",
        "    unique_words = set(doc_words)\n",
        "    for word in unique_words:\n",
        "        exsit_list = word_doc_list[word]\n",
        "        exsit_list.append(i)\n",
        "        word_doc_list[word] = exsit_list\n",
        "\n",
        "#document frequency\n",
        "word_doc_freq = {}\n",
        "for word, doc_list in word_doc_list.items():\n",
        "    word_doc_freq[word] = len(doc_list)\n",
        "\n",
        "# term frequency\n",
        "doc_word_freq = {}\n",
        "\n",
        "for doc_id in range(len(tokenize_sentences)):\n",
        "    words = tokenize_sentences[doc_id]\n",
        "    for word in words:\n",
        "        word_id = word_id_map[word]\n",
        "        doc_word_str = str(doc_id) + ',' + str(word_id)\n",
        "        if doc_word_str in doc_word_freq:\n",
        "            doc_word_freq[doc_word_str] += 1\n",
        "        else:\n",
        "            doc_word_freq[doc_word_str] = 1\n",
        "for i in range(len(tokenize_sentences)):\n",
        "    words = tokenize_sentences[i]\n",
        "    doc_word_set = set()\n",
        "    for word in words:\n",
        "        if word in doc_word_set:\n",
        "            continue\n",
        "        j = word_id_map[word]\n",
        "        key = str(i) + ',' + str(j)\n",
        "        freq = doc_word_freq[key]\n",
        "        if i < train_size:\n",
        "            row.append(i)\n",
        "        else:\n",
        "            row.append(i + vocab_length)\n",
        "        col.append(train_size + j)\n",
        "        idf = log(1.0 * len(tokenize_sentences) / word_doc_freq[word_list[j]])\n",
        "        weight.append(freq * idf)\n",
        "        doc_word_set.add(word)\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Clean up memory after the task\n",
        "gc.collect()\n",
        "# Get current memory usage in bytes\n",
        "mem_after = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
        "\n",
        "time_taken = end_time - start_time\n",
        "memory_used = mem_after - mem_before # Difference in max resident set size\n",
        "\n",
        "print(f\"Time taken for doc-word: {time_taken:.4f} seconds\")\n",
        "print(f\"Memory used for doc-word: {memory_used} bytes\")\n",
        "print(f\"Memory used for doc-word (MB): {memory_used / (1024 * 1024):.4f}\")\n"
      ],
      "metadata": {
        "id": "OEfnDnL_D1fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAr6ygKhWTc-"
      },
      "source": [
        "### doc-doc: jaccard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "b8ed3e5daab54cb59d188047ce073374",
            "4200c73135e5450bb3b3595722f08e10",
            "479327a06815455c9127e029d84a9151",
            "66b12391239c4b628faccf485da9de6f",
            "62a9f6a3aeb942eeab2c4d319010ea0d",
            "bb427ae58a454e368ac23ce3dfc49278",
            "79a2b2aba27149a4a12094775bf82338",
            "cee0f61cdb3b4798bad47aaff64100ba",
            "63e4aa9cd5954d1eb903f3fad890c868",
            "4879f1adae214efc9afb4a81f1c6f31c",
            "754a4bd6e8914e0cad33cdd59fffa0e8"
          ]
        },
        "id": "T4-EH15oWWSX",
        "outputId": "6dd69447-7167-4ca8-ff7c-3efd9c869147"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b8ed3e5daab54cb59d188047ce073374"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "tokenize_sentences_set = [set(s) for s in tokenize_sentences]\n",
        "jaccard_threshold = 0.2\n",
        "for i in tqdm(range(len(tokenize_sentences))):\n",
        "    for j in range(i+1, len(tokenize_sentences)):\n",
        "        # Check if either set is empty before calculating Jaccard distance\n",
        "        if not tokenize_sentences_set[i] or not tokenize_sentences_set[j]:\n",
        "            jaccard_w = 0.0  # or any other default value you prefer\n",
        "        else:\n",
        "            jaccard_w = 1 - nltk.jaccard_distance(tokenize_sentences_set[i], tokenize_sentences_set[j])\n",
        "\n",
        "        if jaccard_w > jaccard_threshold:\n",
        "            if i < train_size:\n",
        "                row.append(i)\n",
        "            else:\n",
        "                row.append(i + vocab_length)\n",
        "            if j < train_size:\n",
        "                col.append(j)\n",
        "            else:\n",
        "                col.append(vocab_length + j)\n",
        "            weight.append(jaccard_w)\n",
        "            if j < train_size:\n",
        "                row.append(j)\n",
        "            else:\n",
        "                row.append(j + vocab_length)\n",
        "            if i < train_size:\n",
        "                col.append(i)\n",
        "            else:\n",
        "                col.append(vocab_length + i)\n",
        "            weight.append(jaccard_w)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## paper-paper moemory and time check"
      ],
      "metadata": {
        "id": "AjGPo3w0ENtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: capture memory usage and woring time for paper-paper\n",
        "\n",
        "%%capture --no-stderr --no-display\n",
        "\n",
        "# Clean up memory before starting the measurement\n",
        "gc.collect()\n",
        "# Get current memory usage in bytes (Linux/macOS)\n",
        "# On some systems, ru_maxrss is in kilobytes, on others in bytes.\n",
        "# Check your system's resource.getrusage documentation.\n",
        "# We'll assume bytes for now, convert to KB or MB for display.\n",
        "try:\n",
        "    mem_before = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
        "except:\n",
        "    mem_before = 0 # Fallback if not available or on other OS\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Paste the code block that defines and runs doc-doc calculation here\n",
        "# Starting from the comment # ### doc-doc: jaccard\n",
        "# Make sure to include all necessary variables and imports defined earlier in your script\n",
        "# For example, ensure `tokenize_sentences`, `train_size`, `vocab_length`,\n",
        "# and `nltk.jaccard_distance` are available.\n",
        "# Ensure nltk is imported and the necessary corpus (like 'nltk.metrics.distance') is available if used.\n",
        "\n",
        "# ### doc-doc: jaccard\n",
        "\n",
        "tokenize_sentences_set = [set(s) for s in tokenize_sentences]\n",
        "jaccard_threshold = 0.2\n",
        "for i in tqdm(range(len(tokenize_sentences))):\n",
        "    for j in range(i+1, len(tokenize_sentences)):\n",
        "        # Check if either set is empty before calculating Jaccard distance\n",
        "        if not tokenize_sentences_set[i] or not tokenize_sentences_set[j]:\n",
        "            jaccard_w = 0.0  # or any other default value you prefer\n",
        "        else:\n",
        "            # Make sure nltk.jaccard_distance is available\n",
        "            # If not, you might need: import nltk; nltk.download('nltk_data') or specific corpus\n",
        "            jaccard_w = 1 - nltk.jaccard_distance(tokenize_sentences_set[i], tokenize_sentences_set[j])\n",
        "\n",
        "        if jaccard_w > jaccard_threshold:\n",
        "            if i < train_size:\n",
        "                row.append(i)\n",
        "            else:\n",
        "                row.append(i + vocab_length)\n",
        "            if j < train_size:\n",
        "                col.append(j)\n",
        "            else:\n",
        "                col.append(vocab_length + j)\n",
        "            weight.append(jaccard_w)\n",
        "            if j < train_size:\n",
        "                row.append(j)\n",
        "            else:\n",
        "                row.append(j + vocab_length)\n",
        "            if i < train_size:\n",
        "                col.append(i)\n",
        "            else:\n",
        "                col.append(vocab_length + i)\n",
        "            weight.append(jaccard_w)\n",
        "\n",
        "\n",
        "end_time = time.time()\n",
        "\n",
        "# Clean up memory after the task\n",
        "gc.collect()\n",
        "# Get current memory usage in bytes\n",
        "try:\n",
        "    mem_after = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss\n",
        "except:\n",
        "    mem_after = 0\n",
        "\n",
        "time_taken = end_time - start_time\n",
        "memory_used = mem_after - mem_before # Difference in max resident set size\n",
        "\n",
        "print(f\"Time taken for doc-doc: {time_taken:.4f} seconds\")\n",
        "print(f\"Memory used for doc-doc: {memory_used} bytes\")\n",
        "print(f\"Memory used for doc-doc (MB): {memory_used / (1024 * 1024):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "566040d57cfc43f7be16b31c30a321aa",
            "72b17c729658444fadb3432e82fa28fd",
            "41ad5ba280f242388842ae0bf9e63401",
            "e6be34bdfbcf4588b5221cefeac56324",
            "32618fb9dfea4f2da30fe7db9493a290",
            "2d3cc2b7534c461aa303c08ca9c55a0f",
            "521ed81c3e6242879d51dca501a6060f",
            "b841ddcdeb224dfc801b8a070382e13c",
            "6e0d449aed844076b14fff3a4f209c0b",
            "7415b0b02f7547ef8fd2529035b155ab",
            "8d5fb2a40a4242e48218c5be7ed8800a"
          ]
        },
        "id": "Ma6JbD2IEEFS",
        "outputId": "40651925-5d52-4e94-e1e5-2f65e0ff36f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/10000 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "566040d57cfc43f7be16b31c30a321aa"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Count total doc to word, word to word, and doc to doc edges"
      ],
      "metadata": {
        "id": "rrNqTq1MCxU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: count doc to word and doc to doc edges togather\n",
        "# Count document-to-word and word-to-word edges\n",
        "doc_to_word_edges = 0\n",
        "word_to_word_edges = 0\n",
        "\n",
        "# Iterate through the 'row' list to identify edge types\n",
        "for i in range(len(row)):\n",
        "    if row[i] < train_size or row[i] >= train_size + vocab_length:  # Check if the source node is a document\n",
        "        doc_to_word_edges += 1\n",
        "    else:\n",
        "        word_to_word_edges += 1\n",
        "\n",
        "print(\"Number of document-to-word edges:\", doc_to_word_edges)\n",
        "print(\"Number of word-to-word edges:\", word_to_word_edges)\n",
        "\n",
        "# Count doc-doc edges\n",
        "doc_doc_edges = 0\n",
        "for i in range(len(row)):\n",
        "    if row[i] < train_size or row[i] >= train_size + vocab_length:\n",
        "        if col[i] < train_size or col[i] >= train_size + vocab_length:\n",
        "            doc_doc_edges += 1\n",
        "\n",
        "print(f\"Number of doc-doc edges: {doc_doc_edges}\")\n",
        "\n",
        "\n",
        "\n",
        "# Total edges\n",
        "total_edges = doc_to_word_edges + word_to_word_edges + doc_doc_edges\n",
        "print(f\"Total number of edges (doc-word + doc-doc): {total_edges}\")"
      ],
      "metadata": {
        "id": "3PMUSj9QB5gH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f961f0c-cf7a-4a54-ea04-e2afc47fe11e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of document-to-word edges: 1758512\n",
            "Number of word-to-word edges: 23905348\n",
            "Number of doc-doc edges: 708\n",
            "Total number of edges (doc-word + doc-doc): 25664568\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uIkGgB2aZDk7"
      },
      "source": [
        "### Adjacent matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C0O1Ucdhod9a"
      },
      "outputs": [],
      "source": [
        "import scipy.sparse as sp\n",
        "adj = sp.csr_matrix((weight, (row, col)), shape=(node_size, node_size))\n",
        "\n",
        "# build symmetric adjacency matrix\n",
        "adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDmGCs3nsqnc"
      },
      "outputs": [],
      "source": [
        "# num_docs = len(sentences[:10]) # num_docs should be the number of documents not the documents themselves\n",
        "\n",
        "# # Print the adjacency matrix with labels\n",
        "# adj_dense = adj.toarray()\n",
        "\n",
        "# print(\"Adjacency Matrix:\")\n",
        "# print(adj_dense)\n",
        "\n",
        "# # Write the adjacency matrix and non-zero elements to a file\n",
        "# with open('adjacency_matrix.txt', 'w') as f:\n",
        "#     f.write(\"Adjacency Matrix:\\n\")\n",
        "#     np.savetxt(f, adj_dense, fmt='%.2f')\n",
        "\n",
        "#     f.write(\"\\nNon-zero elements in the adjacency matrix with labels:\\n\")\n",
        "#     rows, cols = adj.nonzero()\n",
        "#     for r, c in zip(rows, cols):\n",
        "#         r_label = \"Doc\" if r < num_docs else \"Word\"\n",
        "#         c_label = \"Doc\" if c < num_docs else \"Word\"\n",
        "#         f.write(f\"({r_label} {r if r < num_docs else r - num_docs}, {c_label} {c if c < num_docs else c - num_docs}) -> {adj[r, c]}\\n\")\n",
        "\n",
        "# print(\"Data written to adjacency_matrix_with_labels.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fvwruy1tMTb5"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "\n",
        "# num_docs = len(sentences)  # Number of documents  # Use the list of unique words\n",
        "\n",
        "# # Convert adjacency matrix to dense format for easy manipulation\n",
        "# adj_dense = adj.toarray()\n",
        "\n",
        "# print(\"Adjacency Matrix:\")\n",
        "# print(adj_dense)\n",
        "\n",
        "# # Write the adjacency matrix and non-zero elements to a file\n",
        "# with open('adjacency_matrix_with_id.txt', 'w') as f:\n",
        "#     # f.write(\"Adjacency Matrix:\\n\")\n",
        "#     # np.savetxt(f, adj_dense, fmt='%.2f')\n",
        "\n",
        "#     f.write(\"\\nNon-zero elements in the adjacency matrix with ids:\\n\")\n",
        "#     rows, cols = adj.nonzero()\n",
        "#     for r, c in zip(rows, cols):\n",
        "#         r_label = f\"Doc_id: {r}, {sentences[r]}\" if r < num_docs else f\"Word: {word_id_map.get(c - num_docs, c)}, {word_list[r - num_docs]}\"  # Access the list of unique words\n",
        "#         c_label = f\"Doc_id: {c}, {sentences[c]}\" if c < num_docs else f\"Word: {word_id_map.get(c - num_docs, c)}, {word_list[c - num_docs]}\"  # Access the list of unique words\n",
        "#         # Only subtract num_docs if it's a word index\n",
        "#         #c_label = f\"Doc_id {c}\" if c < num_docs else f\"Word {word_id_map.get(c - num_docs, c)}\"  # Use get with a default value to handle potential missing keys\n",
        "#         f.write(f\"({r_label}, {c_label}) -> {adj[r, c]}\\n\")\n",
        "#         print(f\"({r_label}, {c_label}) -> {adj[r, c]}\")\n",
        "\n",
        "# print(\"Data written to adjacency_matrix_with_labels.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ivyuexATkQFW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe606adf-edc3-44e6-d7ac-2f8ee7ee1b84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-29-936935686>:20: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:644.)\n",
            "  return torch.sparse.FloatTensor(indices, values, shape).to(device)\n"
          ]
        }
      ],
      "source": [
        "def normalize_adj(adj):\n",
        "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
        "    adj = sp.coo_matrix(adj)\n",
        "    rowsum = np.array(adj.sum(1))\n",
        "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
        "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
        "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
        "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo(), d_inv_sqrt\n",
        "\n",
        "adj, norm_item = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "    indices = torch.from_numpy(\n",
        "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "    values = torch.from_numpy(sparse_mx.data)\n",
        "    shape = torch.Size(sparse_mx.shape)\n",
        "    return torch.sparse.FloatTensor(indices, values, shape).to(device)\n",
        "\n",
        "adj = sparse_mx_to_torch_sparse_tensor(adj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pMgbhTstMSUA"
      },
      "source": [
        "## Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mP9dqCskOrXT"
      },
      "outputs": [],
      "source": [
        "if NODE == 0:\n",
        "    features = np.arange(node_size)\n",
        "    features = torch.FloatTensor(features).to(device)\n",
        "else:\n",
        "\n",
        "    from flair.embeddings import TransformerDocumentEmbeddings, TransformerWordEmbeddings\n",
        "    from flair.data import Sentence\n",
        "    doc_embedding = TransformerDocumentEmbeddings('bert-base-uncased', fine_tune=False)\n",
        "    word_embedding = TransformerWordEmbeddings('bert-base-uncased', layers='-1',subtoken_pooling=\"mean\")\n",
        "\n",
        "    sent_embs = []\n",
        "    word_embs = {}\n",
        "\n",
        "    for ind in tqdm(range(train_size+test_size)):\n",
        "        sent = tokenize_sentences[ind]\n",
        "        sentence = Sentence(\" \".join(sent[:512]),use_tokenizer=False)\n",
        "        doc_embedding.embed(sentence)\n",
        "        sent_embs.append(sentence.get_embedding().tolist())\n",
        "        words = Sentence(\" \".join(sent[:512]),use_tokenizer=False)\n",
        "        word_embedding.embed(words)\n",
        "        for token in words:\n",
        "            word = token.text\n",
        "            embedding = token.embedding.tolist()\n",
        "            if word not in word_embs:\n",
        "                word_embs[word] = embedding\n",
        "            else:\n",
        "                word_embs[word] = np.minimum(word_embs[word], embedding)\n",
        "\n",
        "    word_embs_list = []\n",
        "    for word in word_list:\n",
        "        word_embs_list.append(word_embs[word])\n",
        "\n",
        "    features = sent_embs[:train_size] + word_embs_list + sent_embs[train_size:]\n",
        "\n",
        "    import scipy.sparse as sp\n",
        "    def preprocess_features(features):\n",
        "        \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "        rowsum = np.array(features.sum(1))\n",
        "        r_inv = np.power(rowsum, -1).flatten()\n",
        "        r_inv[np.isinf(r_inv)] = 0.\n",
        "        r_mat_inv = sp.diags(r_inv)\n",
        "        features = r_mat_inv.dot(features)\n",
        "        return features\n",
        "\n",
        "    features = preprocess_features(sp.csr_matrix(features)).todense()\n",
        "    features = torch.FloatTensor(features).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlVQfOO9CQIp"
      },
      "source": [
        "# Word2Vec Feature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ae3Ygdi0DBpr"
      },
      "outputs": [],
      "source": [
        "# vector_sizes=100\n",
        "# windows=5\n",
        "# min_counts=1\n",
        "# workers=5\n",
        "# epochs=100\n",
        "# cbow = 0\n",
        "# sg = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D8QIM6B8COgV"
      },
      "outputs": [],
      "source": [
        "# if NODE == 0:\n",
        "#     features = np.arange(node_size)\n",
        "#     features = torch.FloatTensor(features).to(device)\n",
        "# else:\n",
        "#     from gensim.models import Word2Vec\n",
        "#     import numpy as np\n",
        "#     from tqdm import tqdm\n",
        "#     import torch\n",
        "\n",
        "#     # Load your Word2Vec model\n",
        "#     #word2vec_model = Word2Vec.load(\"path/to/your/word2vec/model\")\n",
        "#     word2vec_model = Word2Vec(sentences=all_input, vector_size=vector_sizes, window=windows, min_count=min_counts, workers=workers, epochs=epochs, Sg=Sg)\n",
        "#     word2vec_model.wv.save_word2vec_format('CBOW-300-dim.txt', binary=False)\n",
        "\n",
        "#     def get_word_embedding(word, model):\n",
        "#         if word in model.wv:\n",
        "#             return model.wv[word]\n",
        "#         else:\n",
        "#             return np.zeros(model.vector_size)\n",
        "\n",
        "#     sent_embs = []\n",
        "#     word_embs = {}\n",
        "\n",
        "#     for ind in tqdm(range(train_size + test_size)):\n",
        "#         sent = tokenize_sentences[ind]\n",
        "#         sentence_embeddings = []\n",
        "#         for word in sent[:512]:\n",
        "#             word_emb = get_word_embedding(word, word2vec_model)\n",
        "#             sentence_embeddings.append(word_emb)\n",
        "\n",
        "#         if sentence_embeddings:\n",
        "#             sentence_embedding = np.mean(sentence_embeddings, axis=0)\n",
        "#         else:\n",
        "#             sentence_embedding = np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "#         sent_embs.append(sentence_embedding.tolist())\n",
        "\n",
        "#         for word in sent[:512]:\n",
        "#             word_emb = get_word_embedding(word, word2vec_model)\n",
        "#             if word not in word_embs:\n",
        "#                 word_embs[word] = word_emb\n",
        "#             else:\n",
        "#                 word_embs[word] = np.minimum(word_embs[word], word_emb)\n",
        "\n",
        "#     word_embs_list = []\n",
        "#     for word in word_list:\n",
        "#         if word in word_embs:\n",
        "#             word_embs_list.append(word_embs[word])\n",
        "#         else:\n",
        "#             word_embs_list.append(np.zeros(word2vec_model.vector_size))\n",
        "\n",
        "#     features = sent_embs[:train_size] + word_embs_list + sent_embs[train_size:]\n",
        "\n",
        "#     import scipy.sparse as sp\n",
        "#     def preprocess_features(features):\n",
        "#         \"\"\"Row-normalize feature matrix and convert to tuple representation\"\"\"\n",
        "#         rowsum = np.array(features.sum(1))\n",
        "#         r_inv = np.power(rowsum, -1).flatten()\n",
        "#         r_inv[np.isinf(r_inv)] = 0.\n",
        "#         r_mat_inv = sp.diags(r_inv)\n",
        "#         features = r_mat_inv.dot(features)\n",
        "#         return features\n",
        "\n",
        "#     features = preprocess_features(sp.csr_matrix(features)).todense()\n",
        "#     features = torch.FloatTensor(features).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0KJyPVPpmll"
      },
      "outputs": [],
      "source": [
        "# !pip install openai\n",
        "# !pip install langchain\n",
        "# !pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXvfJNu3o-_F"
      },
      "outputs": [],
      "source": [
        "# import openai\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# import os\n",
        "\n",
        "# from langchain.embeddings import OpenAIEmbeddings\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
        "\n",
        "\n",
        "# def get_openai_embedding(text):\n",
        "#     # Update to use the new API client\n",
        "#     client = openai.OpenAI() # Initialize the OpenAI client\n",
        "#     response = client.embeddings.create(input=text, model=\"text-embedding-ada-002\")\n",
        "#     return response.data[0].embedding\n",
        "\n",
        "# sent_embs = []\n",
        "# word_embs = {}\n",
        "\n",
        "# # Obtain sentence embeddings and build word embeddings\n",
        "# for ind in tqdm(range(train_size + test_size)):\n",
        "#     sent = tokenize_sentences[ind]\n",
        "#     sentence_embedding = get_openai_embedding(\" \".join(sent[:1536]))\n",
        "#     sent_embs.append(sentence_embedding)\n",
        "\n",
        "#     # Process word embeddings\n",
        "#     for word in sent[:1536]:\n",
        "#         word_embedding = get_openai_embedding(word)\n",
        "#         if word not in word_embs:\n",
        "#             word_embs[word] = word_embedding\n",
        "#         else:\n",
        "#             word_embs[word] = np.minimum(word_embs[word], word_embedding)\n",
        "\n",
        "# # Create a list of word embeddings\n",
        "# word_embs_list = [word_embs[word] if word in word_embs else np.zeros(len(sentence_embedding)) for word in word_list]\n",
        "\n",
        "# # Combine features\n",
        "# features = sent_embs[:train_size] + word_embs_list + sent_embs[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ernmG5Z-9mc-"
      },
      "outputs": [],
      "source": [
        "# # prompt: download this csv file after complete\n",
        "\n",
        "# import pandas as pd\n",
        "# from google.colab import files\n",
        "\n",
        "# # Assuming df is defined in your code and contains the word embeddings\n",
        "# df = pd.DataFrame(word_embs_list)\n",
        "# df.to_csv('ieee_word_embed.csv', index=False)\n",
        "\n",
        "# # Download the file\n",
        "# files.download('ieee_word_embed.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdx6RrUvjbF0"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39Kj8NQujiDH"
      },
      "source": [
        "## GCN Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jNVkA-h7b3sP"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "import torch\n",
        "\n",
        "from torch.nn.parameter import Parameter\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "\n",
        "class GraphConvolution(Module):\n",
        "    \"\"\"\n",
        "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_features, out_features,  drop_out = 0, activation=None, bias=True):\n",
        "        super(GraphConvolution, self).__init__()\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
        "        if bias:\n",
        "            self.bias = Parameter(torch.zeros(1, out_features))\n",
        "        else:\n",
        "            self.register_parameter('bias', None)\n",
        "        self.reset_parameters(in_features, out_features)\n",
        "        self.dropout = torch.nn.Dropout(drop_out)\n",
        "        self.activation =  activation\n",
        "\n",
        "    def reset_parameters(self,in_features, out_features):\n",
        "        stdv = np.sqrt(6.0/(in_features+out_features))\n",
        "        # stdv = 1. / math.sqrt(self.weight.size(1))\n",
        "        self.weight.data.uniform_(-stdv, stdv)\n",
        "        # if self.bias is not None:\n",
        "        #     torch.nn.init.zeros_(self.bias)\n",
        "            # self.bias.data.uniform_(-stdv, stdv)\n",
        "\n",
        "\n",
        "    def forward(self, input, adj, feature_less = False):\n",
        "        if feature_less:\n",
        "            support = self.weight\n",
        "            support = self.dropout(support)\n",
        "        else:\n",
        "            input = self.dropout(input)\n",
        "            support = torch.mm(input, self.weight)\n",
        "        output = torch.spmm(adj, support)\n",
        "        if self.bias is not None:\n",
        "            output = output + self.bias\n",
        "        if self.activation is not None:\n",
        "            output = self.activation(output)\n",
        "        return output\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + ' (' \\\n",
        "               + str(self.in_features) + ' -> ' \\\n",
        "               + str(self.out_features) + ')'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k57M4sz4s4Md"
      },
      "source": [
        "## GCN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJ-ZQuMzs5tZ"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class GCN(nn.Module):\n",
        "    def __init__(self, nfeat, nhid, nclass, dropout, n_layers = 2):\n",
        "        super(GCN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.gc_list = []\n",
        "        if n_layers >= 2:\n",
        "            self.gc1 = GraphConvolution(nfeat, nhid, dropout, activation = nn.ReLU())\n",
        "            self.gc_list = nn.ModuleList([GraphConvolution(nhid, nhid, dropout, activation = nn.ReLU()) for _ in range(self.n_layers-2)])\n",
        "            self.gcf = GraphConvolution(nhid, nclass, dropout)\n",
        "        else:\n",
        "            self.gc1 = GraphConvolution(nfeat, nclass, dropout)\n",
        "\n",
        "    def forward(self, x, adj):\n",
        "        if self.n_layers>=2:\n",
        "            x = self.gc1(x, adj, feature_less = True)\n",
        "            for i in range(self.n_layers-2):\n",
        "                x = self.gc_list[i](x,adj)\n",
        "            x = self.gcf(x,adj)\n",
        "        else:\n",
        "            x = self.gc1(x, adj, feature_less = True)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qmhOG1yG--Ji"
      },
      "outputs": [],
      "source": [
        "def cal_accuracy(predictions,labels):\n",
        "    pred = torch.argmax(predictions,-1).cpu().tolist()\n",
        "    lab = labels.cpu().tolist()\n",
        "    cor = 0\n",
        "    for i in range(len(pred)):\n",
        "        if pred[i] == lab[i]:\n",
        "            cor += 1\n",
        "    return cor/len(pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEE4JxeUthCb"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIxII4QoticA"
      },
      "source": [
        "## Initialize model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch.optim as optim\n",
        "# import time\n",
        "# import csv\n",
        "# import torch\n",
        "# from torch_geometric.data import Data\n",
        "\n",
        "# # Define num_class with the correct value\n",
        "# num_class = 10  # Assume you have 10 classes, replace with your actual number of classes.\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT, n_layers=NUM_LAYERS).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "\n",
        "# def train_model(show_result=True):\n",
        "#     val_loss = []\n",
        "#     train_loss = []  # List to store training losses\n",
        "#     train_acc = []  # List to store training accuracies\n",
        "#     val_acc = []  # List to store validation accuracies\n",
        "\n",
        "#     # Ensure labels are on the correct device\n",
        "#     labels_tensor = labels.to(device).long()  # Ensure labels are long type\n",
        "\n",
        "#     for epoch in range(NUM_EPOCHS):\n",
        "#         t = time.time()\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Create a PyTorch Geometric Data object\n",
        "#         # Ensure edge_index is a LongTensor and on the correct device\n",
        "#         # Fix: Ensure edge_index has shape [2, num_edges] and valid values\n",
        "\n",
        "#         # Get edge indices as a dense NumPy array\n",
        "#         edge_index_np = adj.coalesce().indices().cpu().numpy()\n",
        "\n",
        "#         # Check for invalid node indices in edge_index\n",
        "#         num_nodes = features.shape[0]  # Get the number of nodes\n",
        "#         if np.any(edge_index_np >= num_nodes):\n",
        "#             print(\"Warning: Invalid node indices found in edge_index. Clipping to valid range.\")\n",
        "#             edge_index_np = np.clip(edge_index_np, 0, num_nodes - 1)\n",
        "\n",
        "#         # Convert back to a PyTorch LongTensor and move to device\n",
        "#         edge_index = torch.tensor(edge_index_np, dtype=torch.long, device=device)\n",
        "\n",
        "#         data = Data(x=features, edge_index=edge_index)  # Pass features and edge_index to Data\n",
        "\n",
        "#         output = model(data)  # Pass the Data object to the model\n",
        "#         loss_train_epoch = criterion(output[idx_train], labels_tensor[idx_train])\n",
        "#         acc_train_epoch = cal_accuracy(output[idx_train], labels_tensor[idx_train])\n",
        "#         loss_train_epoch.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Store training loss and accuracy for this epoch\n",
        "#         train_loss.append(loss_train_epoch.item())\n",
        "#         train_acc.append(acc_train_epoch)\n",
        "\n",
        "#         model.eval()\n",
        "#         # Fix: Pass Data object to model during evaluation\n",
        "#         output = model(data)\n",
        "\n",
        "#         loss_val_epoch = criterion(output[idx_val], labels_tensor[idx_val])\n",
        "#         val_loss.append(loss_val_epoch.item())\n",
        "#         acc_val_epoch = cal_accuracy(output[idx_val], labels_tensor[idx_val])\n",
        "\n",
        "#         # Store validation accuracy for this epoch\n",
        "#         val_acc.append(acc_val_epoch)\n",
        "\n",
        "#         if show_result:\n",
        "#             print('Epoch: {:04d}'.format(epoch + 1),\n",
        "#                   'loss_train: {:.4f}'.format(loss_train_epoch.item()),\n",
        "#                   'acc_train: {:.4f}'.format(acc_train_epoch),\n",
        "#                   'loss_val: {:.4f}'.format(loss_val_epoch.item()),\n",
        "#                   'acc_val: {:.4f}'.format(acc_val_epoch),\n",
        "#                   'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "#         if epoch > EARLY_STOPPING and np.min(val_loss[-EARLY_STOPPING:]) > np.min(val_loss[:-EARLY_STOPPING]):\n",
        "#             if show_result:\n",
        "#                 print(\"Early Stopping...\")\n",
        "#             break\n",
        "\n",
        "#     return train_loss, train_acc, val_loss, val_acc  # Return the collected metrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "P9tnz84VXT8U",
        "outputId": "05197e46-6f8e-4e63-a168-e0d4c3d383d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch_geometric'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-458692530>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Define num_class with the correct value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2차원 라벨 모델"
      ],
      "metadata": {
        "id": "PkN35A2G-J5r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hdNsgxMG-Wwu"
      },
      "outputs": [],
      "source": [
        "# import torch.optim as optim\n",
        "\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\n",
        "# optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 차원 라벨 모델"
      ],
      "metadata": {
        "id": "p7X-DXPb-DvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Define num_class with the correct value\n",
        "num_class = 5 # Assume you have 10 classes, replace with your actual number of classes.\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)"
      ],
      "metadata": {
        "id": "jBS6xiJn8CDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T98r4qZuuFyn"
      },
      "source": [
        "## Training and Validating"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bv9br9pgGw9R"
      },
      "outputs": [],
      "source": [
        "def generate_train_val(train_pro=0.9):\n",
        "    real_train_size = int(train_pro*train_size)\n",
        "    val_size = train_size-real_train_size\n",
        "\n",
        "    idx_train = np.random.choice(train_size, real_train_size,replace=False)\n",
        "    idx_train.sort()\n",
        "    idx_val = []\n",
        "    pointer = 0\n",
        "    for v in range(train_size):\n",
        "        if pointer<len(idx_train) and idx_train[pointer] == v:\n",
        "            pointer +=1\n",
        "        else:\n",
        "            idx_val.append(v)\n",
        "    idx_test = range(train_size+vocab_length, node_size)\n",
        "    return idx_train, idx_val, idx_test\n",
        "\n",
        "idx_train, idx_val, idx_test = generate_train_val()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ld1txX3sR0J3"
      },
      "source": [
        "# Model Training"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import time\n",
        "# import csv\n",
        "# import torch\n",
        "# from torch_geometric.data import Data\n",
        "\n",
        "# def train_model(show_result=True):\n",
        "#     val_loss = []\n",
        "#     train_loss = []  # List to store training losses\n",
        "#     train_acc = []  # List to store training accuracies\n",
        "#     val_acc = []  # List to store validation accuracies\n",
        "\n",
        "#     # Ensure labels are on the correct device\n",
        "#     labels_tensor = labels.to(device)  # Move labels to the desired device if needed\n",
        "\n",
        "#     for epoch in range(NUM_EPOCHS):\n",
        "#         t = time.time()\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         # Create a PyTorch Geometric Data object\n",
        "#         # Ensure edge_index is a LongTensor and on the correct device\n",
        "#         # Fix: Ensure edge_index has shape [2, num_edges] and valid values\n",
        "\n",
        "#         labels_tensor = labels.to(device).long()  # Ensure labels are long type\n",
        "#         # Get edge indices as a dense NumPy array\n",
        "#         edge_index_np = adj.coalesce().indices().cpu().numpy()\n",
        "\n",
        "#         # Check for invalid node indices in edge_index\n",
        "#         num_nodes = features.shape[0]  # Get the number of nodes\n",
        "#         if np.any(edge_index_np >= num_nodes):\n",
        "#             print(\"Warning: Invalid node indices found in edge_index. Clipping to valid range.\")\n",
        "#             edge_index_np = np.clip(edge_index_np, 0, num_nodes - 1)\n",
        "\n",
        "#         # Convert back to a PyTorch LongTensor and move to device\n",
        "#         edge_index = torch.tensor(edge_index_np, dtype=torch.long, device=device)\n",
        "\n",
        "#         data = Data(x=features, edge_index=edge_index)\n",
        "\n",
        "#         output = model(data)  # Pass the Data object to the model\n",
        "#         loss_train_epoch = criterion(output[idx_train], labels_tensor[idx_train])\n",
        "#         acc_train_epoch = cal_accuracy(output[idx_train], labels_tensor[idx_train])\n",
        "#         loss_train_epoch.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Store training loss and accuracy for this epoch\n",
        "#         train_loss.append(loss_train_epoch.item())\n",
        "#         train_acc.append(acc_train_epoch)\n",
        "\n",
        "#         model.eval()\n",
        "#         # Fix: Pass Data object to model during evaluation\n",
        "#         output = model(data)\n",
        "\n",
        "#         loss_val_epoch = criterion(output[idx_val], labels_tensor[idx_val])\n",
        "#         val_loss.append(loss_val_epoch.item())\n",
        "#         acc_val_epoch = cal_accuracy(output[idx_val], labels_tensor[idx_val])\n",
        "\n",
        "#         # Store validation accuracy for this epoch\n",
        "#         val_acc.append(acc_val_epoch)\n",
        "\n",
        "#         if show_result:\n",
        "#             print(  'Epoch: {:04d}'.format(epoch+1),\n",
        "#                     'loss_train: {:.4f}'.format(loss_train_epoch.item()),\n",
        "#                     'acc_train: {:.4f}'.format(acc_train_epoch),\n",
        "#                     'loss_val: {:.4f}'.format(loss_val_epoch.item()),\n",
        "#                     'acc_val: {:.4f}'.format(acc_val_epoch),\n",
        "#                     'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "#         if epoch > EARLY_STOPPING and np.min(val_loss[-EARLY_STOPPING:]) > np.min(val_loss[:-EARLY_STOPPING]) :\n",
        "#             if show_result:\n",
        "#                 print(\"Early Stopping...\")\n",
        "#             break\n",
        "\n",
        "#     return train_loss, train_acc, val_loss, val_acc # Return the collected metrics\n",
        "\n",
        "# # Call train_model and store the returned metrics\n",
        "# loss_train, acc_train, loss_val, acc_val = train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "DehSPDlOQKY8",
        "outputId": "63fcfa38-ff2f-4c05-8c3f-4cd541cccc67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torch_geometric'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-3261022398>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch_geometric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshow_result\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch_geometric'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2차원 라벨으로 모델 학습"
      ],
      "metadata": {
        "id": "4vdrPSEw984Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WAoPNJd5P3b"
      },
      "outputs": [],
      "source": [
        "# import time\n",
        "# import csv\n",
        "\n",
        "# def train_model(show_result = True):\n",
        "#     val_loss = []\n",
        "#     train_loss = [] # List to store training losses\n",
        "#     train_acc = []  # List to store training accuracies\n",
        "#     val_acc = []    # List to store validation accuracies\n",
        "#     # Convert labels to a tensor once at the beginning\n",
        "#     labels_tensor = torch.LongTensor(labels).to(device)  # Assuming 'device' is defined elsewhere\n",
        "\n",
        "#     for epoch in range(NUM_EPOCHS):\n",
        "#         t = time.time()\n",
        "#         model.train()\n",
        "#         optimizer.zero_grad()\n",
        "#         output= model(features, adj)\n",
        "#         loss_train_epoch = criterion(output[idx_train], labels_tensor[idx_train]) # Use labels_tensor\n",
        "#         acc_train_epoch = cal_accuracy(output[idx_train], labels_tensor[idx_train]) # Use labels_tensor\n",
        "#         loss_train_epoch.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Store training loss and accuracy for this epoch\n",
        "#         train_loss.append(loss_train_epoch.item())\n",
        "#         train_acc.append(acc_train_epoch)\n",
        "\n",
        "#         model.eval()\n",
        "#         output = model(features, adj)\n",
        "\n",
        "#         loss_val_epoch = criterion(output[idx_val], labels_tensor[idx_val]) # Use labels_tensor\n",
        "#         val_loss.append(loss_val_epoch.item())\n",
        "#         acc_val_epoch = cal_accuracy(output[idx_val], labels_tensor[idx_val])\n",
        "\n",
        "#         # Store validation accuracy for this epoch\n",
        "#         val_acc.append(acc_val_epoch)\n",
        "\n",
        "#         if show_result:\n",
        "#             print(  'Epoch: {:04d}'.format(epoch+1),\n",
        "#                     'loss_train: {:.4f}'.format(loss_train_epoch.item()),\n",
        "#                     'acc_train: {:.4f}'.format(acc_train_epoch),\n",
        "#                     'loss_val: {:.4f}'.format(loss_val_epoch.item()),\n",
        "#                     'acc_val: {:.4f}'.format(acc_val_epoch),\n",
        "#                     'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "#         if epoch > EARLY_STOPPING and np.min(val_loss[-EARLY_STOPPING:]) > np.min(val_loss[:-EARLY_STOPPING]) :\n",
        "#             if show_result:\n",
        "#                 print(\"Early Stopping...\")\n",
        "#             break\n",
        "\n",
        "#     return train_loss, train_acc, val_loss, val_acc # Return the collected metrics\n",
        "\n",
        "# # Call train_model and store the returned metrics\n",
        "# loss_train, acc_train, loss_val, acc_val = train_model()\n",
        "\n",
        "# # Save metrics to CSV\n",
        "# with open('CS_training_data.csv', mode='w', newline='') as file:\n",
        "#     writer = csv.writer(file)\n",
        "#     writer.writerow(['Epoch', 'Train Loss', 'Train Accuracy', 'Validation Loss', 'Validation Accuracy'])\n",
        "#     for i in range(len(loss_train)):\n",
        "#         writer.writerow([i+1, loss_train[i], acc_train[i], loss_val[i], acc_val[i]])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1차원 라벨으로 모델 학습"
      ],
      "metadata": {
        "id": "UwSnuoWh93yt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import csv\n",
        "import torch\n",
        "\n",
        "def train_model(show_result = True):\n",
        "    val_loss = []\n",
        "    train_loss = [] # List to store training losses\n",
        "    train_acc = []  # List to store training accuracies\n",
        "    val_acc = []    # List to store validation accuracies\n",
        "\n",
        "    # Ensure labels are on the correct device\n",
        "    labels_tensor = labels.to(device)  # Move labels to the desired device if needed\n",
        "    #If labels is already a CUDA tensor, this line will have no effect, but won't cause harm either.\n",
        "    # If labels is a CPU tensor, it will be moved to the GPU.\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        t = time.time()\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output= model(features, adj)\n",
        "        loss_train_epoch = criterion(output[idx_train], labels_tensor[idx_train])\n",
        "        acc_train_epoch = cal_accuracy(output[idx_train], labels_tensor[idx_train])\n",
        "        loss_train_epoch.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store training loss and accuracy for this epoch\n",
        "        train_loss.append(loss_train_epoch.item())\n",
        "        train_acc.append(acc_train_epoch)\n",
        "\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "        loss_val_epoch = criterion(output[idx_val], labels_tensor[idx_val])\n",
        "        val_loss.append(loss_val_epoch.item())\n",
        "        acc_val_epoch = cal_accuracy(output[idx_val], labels_tensor[idx_val])\n",
        "\n",
        "        # Store validation accuracy for this epoch\n",
        "        val_acc.append(acc_val_epoch)\n",
        "\n",
        "        if show_result:\n",
        "            print(  'Epoch: {:04d}'.format(epoch+1),\n",
        "                    'loss_train: {:.4f}'.format(loss_train_epoch.item()),\n",
        "                    'acc_train: {:.4f}'.format(acc_train_epoch),\n",
        "                    'loss_val: {:.4f}'.format(loss_val_epoch.item()),\n",
        "                    'acc_val: {:.4f}'.format(acc_val_epoch),\n",
        "                    'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "        if epoch > EARLY_STOPPING and np.min(val_loss[-EARLY_STOPPING:]) > np.min(val_loss[:-EARLY_STOPPING]) :\n",
        "            if show_result:\n",
        "                print(\"Early Stopping...\")\n",
        "            break\n",
        "\n",
        "    return train_loss, train_acc, val_loss, val_acc # Return the collected metrics\n",
        "\n",
        "# Call train_model and store the returned metrics\n",
        "loss_train, acc_train, loss_val, acc_val = train_model()"
      ],
      "metadata": {
        "id": "4Ya5iDgz8fMD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "803b674d-9529-4057-eb31-ada2eaebf703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 2.3023 acc_train: 0.3311 loss_val: 2.1942 acc_val: 0.9350 time: 1.9184s\n",
            "Epoch: 0002 loss_train: 2.1948 acc_train: 0.9386 loss_val: 2.0614 acc_val: 0.9350 time: 0.7221s\n",
            "Epoch: 0003 loss_train: 2.0625 acc_train: 0.9386 loss_val: 1.8998 acc_val: 0.9350 time: 0.7251s\n",
            "Epoch: 0004 loss_train: 1.9017 acc_train: 0.9386 loss_val: 1.7124 acc_val: 0.9350 time: 0.7267s\n",
            "Epoch: 0005 loss_train: 1.7147 acc_train: 0.9386 loss_val: 1.5042 acc_val: 0.9350 time: 0.7269s\n",
            "Epoch: 0006 loss_train: 1.5073 acc_train: 0.9386 loss_val: 1.2834 acc_val: 0.9350 time: 0.7263s\n",
            "Epoch: 0007 loss_train: 1.2859 acc_train: 0.9386 loss_val: 1.0620 acc_val: 0.9350 time: 0.7273s\n",
            "Epoch: 0008 loss_train: 1.0653 acc_train: 0.9386 loss_val: 0.8553 acc_val: 0.9350 time: 0.7290s\n",
            "Epoch: 0009 loss_train: 0.8593 acc_train: 0.9386 loss_val: 0.6789 acc_val: 0.9350 time: 0.7282s\n",
            "Epoch: 0010 loss_train: 0.6828 acc_train: 0.9386 loss_val: 0.5431 acc_val: 0.9350 time: 0.7299s\n",
            "Epoch: 0011 loss_train: 0.5445 acc_train: 0.9386 loss_val: 0.4489 acc_val: 0.9350 time: 0.7298s\n",
            "Epoch: 0012 loss_train: 0.4488 acc_train: 0.9386 loss_val: 0.3898 acc_val: 0.9350 time: 0.7328s\n",
            "Epoch: 0013 loss_train: 0.3880 acc_train: 0.9386 loss_val: 0.3564 acc_val: 0.9350 time: 0.7341s\n",
            "Epoch: 0014 loss_train: 0.3531 acc_train: 0.9386 loss_val: 0.3394 acc_val: 0.9350 time: 0.7352s\n",
            "Epoch: 0015 loss_train: 0.3353 acc_train: 0.9386 loss_val: 0.3318 acc_val: 0.9350 time: 0.7334s\n",
            "Epoch: 0016 loss_train: 0.3269 acc_train: 0.9386 loss_val: 0.3288 acc_val: 0.9350 time: 0.7357s\n",
            "Epoch: 0017 loss_train: 0.3228 acc_train: 0.9386 loss_val: 0.3276 acc_val: 0.9350 time: 0.7363s\n",
            "Epoch: 0018 loss_train: 0.3210 acc_train: 0.9386 loss_val: 0.3263 acc_val: 0.9350 time: 0.7326s\n",
            "Epoch: 0019 loss_train: 0.3197 acc_train: 0.9386 loss_val: 0.3240 acc_val: 0.9350 time: 0.7358s\n",
            "Epoch: 0020 loss_train: 0.3170 acc_train: 0.9386 loss_val: 0.3204 acc_val: 0.9350 time: 0.7369s\n",
            "Epoch: 0021 loss_train: 0.3132 acc_train: 0.9386 loss_val: 0.3153 acc_val: 0.9350 time: 0.7338s\n",
            "Epoch: 0022 loss_train: 0.3082 acc_train: 0.9386 loss_val: 0.3087 acc_val: 0.9350 time: 0.7344s\n",
            "Epoch: 0023 loss_train: 0.3017 acc_train: 0.9386 loss_val: 0.3008 acc_val: 0.9350 time: 0.7388s\n",
            "Epoch: 0024 loss_train: 0.2941 acc_train: 0.9386 loss_val: 0.2920 acc_val: 0.9350 time: 0.7341s\n",
            "Epoch: 0025 loss_train: 0.2853 acc_train: 0.9386 loss_val: 0.2825 acc_val: 0.9350 time: 0.7368s\n",
            "Epoch: 0026 loss_train: 0.2759 acc_train: 0.9386 loss_val: 0.2728 acc_val: 0.9350 time: 0.7387s\n",
            "Epoch: 0027 loss_train: 0.2665 acc_train: 0.9386 loss_val: 0.2635 acc_val: 0.9350 time: 0.7386s\n",
            "Epoch: 0028 loss_train: 0.2576 acc_train: 0.9386 loss_val: 0.2553 acc_val: 0.9350 time: 0.7382s\n",
            "Epoch: 0029 loss_train: 0.2497 acc_train: 0.9386 loss_val: 0.2487 acc_val: 0.9350 time: 0.7394s\n",
            "Epoch: 0030 loss_train: 0.2435 acc_train: 0.9386 loss_val: 0.2445 acc_val: 0.9350 time: 0.7392s\n",
            "Epoch: 0031 loss_train: 0.2400 acc_train: 0.9387 loss_val: 0.2432 acc_val: 0.9350 time: 0.7422s\n",
            "Epoch: 0032 loss_train: 0.2391 acc_train: 0.9389 loss_val: 0.2447 acc_val: 0.9350 time: 0.7400s\n",
            "Epoch: 0033 loss_train: 0.2410 acc_train: 0.9389 loss_val: 0.2482 acc_val: 0.9350 time: 0.7403s\n",
            "Epoch: 0034 loss_train: 0.2447 acc_train: 0.9389 loss_val: 0.2523 acc_val: 0.9350 time: 0.7417s\n",
            "Epoch: 0035 loss_train: 0.2489 acc_train: 0.9389 loss_val: 0.2551 acc_val: 0.9350 time: 0.7426s\n",
            "Epoch: 0036 loss_train: 0.2522 acc_train: 0.9389 loss_val: 0.2557 acc_val: 0.9350 time: 0.7378s\n",
            "Epoch: 0037 loss_train: 0.2526 acc_train: 0.9389 loss_val: 0.2539 acc_val: 0.9350 time: 0.7434s\n",
            "Epoch: 0038 loss_train: 0.2506 acc_train: 0.9389 loss_val: 0.2506 acc_val: 0.9350 time: 0.7361s\n",
            "Epoch: 0039 loss_train: 0.2474 acc_train: 0.9389 loss_val: 0.2471 acc_val: 0.9350 time: 0.7353s\n",
            "Epoch: 0040 loss_train: 0.2433 acc_train: 0.9389 loss_val: 0.2442 acc_val: 0.9350 time: 0.7347s\n",
            "Epoch: 0041 loss_train: 0.2399 acc_train: 0.9389 loss_val: 0.2426 acc_val: 0.9350 time: 0.7343s\n",
            "Epoch: 0042 loss_train: 0.2382 acc_train: 0.9389 loss_val: 0.2421 acc_val: 0.9350 time: 0.7340s\n",
            "Epoch: 0043 loss_train: 0.2373 acc_train: 0.9389 loss_val: 0.2427 acc_val: 0.9350 time: 0.7343s\n",
            "Epoch: 0044 loss_train: 0.2376 acc_train: 0.9389 loss_val: 0.2438 acc_val: 0.9350 time: 0.7323s\n",
            "Epoch: 0045 loss_train: 0.2382 acc_train: 0.9389 loss_val: 0.2451 acc_val: 0.9350 time: 0.7316s\n",
            "Epoch: 0046 loss_train: 0.2395 acc_train: 0.9389 loss_val: 0.2463 acc_val: 0.9350 time: 0.7328s\n",
            "Epoch: 0047 loss_train: 0.2401 acc_train: 0.9389 loss_val: 0.2470 acc_val: 0.9350 time: 0.7308s\n",
            "Epoch: 0048 loss_train: 0.2408 acc_train: 0.9389 loss_val: 0.2473 acc_val: 0.9350 time: 0.7324s\n",
            "Epoch: 0049 loss_train: 0.2410 acc_train: 0.9387 loss_val: 0.2470 acc_val: 0.9350 time: 0.7311s\n",
            "Epoch: 0050 loss_train: 0.2407 acc_train: 0.9389 loss_val: 0.2463 acc_val: 0.9350 time: 0.7329s\n",
            "Epoch: 0051 loss_train: 0.2400 acc_train: 0.9389 loss_val: 0.2454 acc_val: 0.9350 time: 0.7318s\n",
            "Epoch: 0052 loss_train: 0.2391 acc_train: 0.9389 loss_val: 0.2443 acc_val: 0.9350 time: 0.7334s\n",
            "Epoch: 0053 loss_train: 0.2380 acc_train: 0.9389 loss_val: 0.2432 acc_val: 0.9350 time: 0.7328s\n",
            "Epoch: 0054 loss_train: 0.2371 acc_train: 0.9389 loss_val: 0.2424 acc_val: 0.9350 time: 0.7337s\n",
            "Epoch: 0055 loss_train: 0.2367 acc_train: 0.9389 loss_val: 0.2420 acc_val: 0.9350 time: 0.7298s\n",
            "Epoch: 0056 loss_train: 0.2362 acc_train: 0.9389 loss_val: 0.2418 acc_val: 0.9350 time: 0.7293s\n",
            "Epoch: 0057 loss_train: 0.2362 acc_train: 0.9389 loss_val: 0.2419 acc_val: 0.9350 time: 0.7286s\n",
            "Epoch: 0058 loss_train: 0.2365 acc_train: 0.9389 loss_val: 0.2422 acc_val: 0.9350 time: 0.7287s\n",
            "Epoch: 0059 loss_train: 0.2369 acc_train: 0.9389 loss_val: 0.2424 acc_val: 0.9350 time: 0.7270s\n",
            "Epoch: 0060 loss_train: 0.2369 acc_train: 0.9389 loss_val: 0.2425 acc_val: 0.9350 time: 0.7275s\n",
            "Epoch: 0061 loss_train: 0.2371 acc_train: 0.9389 loss_val: 0.2425 acc_val: 0.9350 time: 0.7278s\n",
            "Epoch: 0062 loss_train: 0.2370 acc_train: 0.9389 loss_val: 0.2423 acc_val: 0.9350 time: 0.7273s\n",
            "Epoch: 0063 loss_train: 0.2369 acc_train: 0.9389 loss_val: 0.2421 acc_val: 0.9350 time: 0.7276s\n",
            "Epoch: 0064 loss_train: 0.2362 acc_train: 0.9389 loss_val: 0.2419 acc_val: 0.9350 time: 0.7267s\n",
            "Epoch: 0065 loss_train: 0.2360 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7262s\n",
            "Epoch: 0066 loss_train: 0.2355 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7258s\n",
            "Epoch: 0067 loss_train: 0.2354 acc_train: 0.9389 loss_val: 0.2418 acc_val: 0.9350 time: 0.7246s\n",
            "Epoch: 0068 loss_train: 0.2352 acc_train: 0.9389 loss_val: 0.2420 acc_val: 0.9350 time: 0.7279s\n",
            "Epoch: 0069 loss_train: 0.2353 acc_train: 0.9389 loss_val: 0.2421 acc_val: 0.9350 time: 0.7277s\n",
            "Epoch: 0070 loss_train: 0.2353 acc_train: 0.9389 loss_val: 0.2423 acc_val: 0.9350 time: 0.7383s\n",
            "Epoch: 0071 loss_train: 0.2354 acc_train: 0.9389 loss_val: 0.2424 acc_val: 0.9350 time: 0.7292s\n",
            "Epoch: 0072 loss_train: 0.2355 acc_train: 0.9389 loss_val: 0.2424 acc_val: 0.9350 time: 0.7242s\n",
            "Epoch: 0073 loss_train: 0.2352 acc_train: 0.9389 loss_val: 0.2423 acc_val: 0.9350 time: 0.7276s\n",
            "Epoch: 0074 loss_train: 0.2350 acc_train: 0.9389 loss_val: 0.2422 acc_val: 0.9350 time: 0.7248s\n",
            "Epoch: 0075 loss_train: 0.2348 acc_train: 0.9389 loss_val: 0.2420 acc_val: 0.9350 time: 0.7238s\n",
            "Epoch: 0076 loss_train: 0.2349 acc_train: 0.9389 loss_val: 0.2418 acc_val: 0.9350 time: 0.7258s\n",
            "Epoch: 0077 loss_train: 0.2347 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7233s\n",
            "Epoch: 0078 loss_train: 0.2346 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7234s\n",
            "Epoch: 0079 loss_train: 0.2345 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7234s\n",
            "Epoch: 0080 loss_train: 0.2344 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7232s\n",
            "Epoch: 0081 loss_train: 0.2343 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7237s\n",
            "Epoch: 0082 loss_train: 0.2343 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7219s\n",
            "Epoch: 0083 loss_train: 0.2341 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7233s\n",
            "Epoch: 0084 loss_train: 0.2341 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7238s\n",
            "Epoch: 0085 loss_train: 0.2341 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7225s\n",
            "Epoch: 0086 loss_train: 0.2339 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7238s\n",
            "Epoch: 0087 loss_train: 0.2337 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7232s\n",
            "Epoch: 0088 loss_train: 0.2338 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7246s\n",
            "Epoch: 0089 loss_train: 0.2338 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7249s\n",
            "Epoch: 0090 loss_train: 0.2337 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7244s\n",
            "Epoch: 0091 loss_train: 0.2335 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7238s\n",
            "Epoch: 0092 loss_train: 0.2336 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7257s\n",
            "Epoch: 0093 loss_train: 0.2333 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7254s\n",
            "Epoch: 0094 loss_train: 0.2332 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7236s\n",
            "Epoch: 0095 loss_train: 0.2331 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7226s\n",
            "Epoch: 0096 loss_train: 0.2330 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7226s\n",
            "Epoch: 0097 loss_train: 0.2331 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7223s\n",
            "Epoch: 0098 loss_train: 0.2328 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7226s\n",
            "Epoch: 0099 loss_train: 0.2326 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7235s\n",
            "Epoch: 0100 loss_train: 0.2329 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7226s\n",
            "Epoch: 0101 loss_train: 0.2326 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7226s\n",
            "Epoch: 0102 loss_train: 0.2323 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7231s\n",
            "Epoch: 0103 loss_train: 0.2326 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7210s\n",
            "Epoch: 0104 loss_train: 0.2324 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7226s\n",
            "Epoch: 0105 loss_train: 0.2323 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7222s\n",
            "Epoch: 0106 loss_train: 0.2322 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7218s\n",
            "Epoch: 0107 loss_train: 0.2321 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7230s\n",
            "Epoch: 0108 loss_train: 0.2320 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7240s\n",
            "Epoch: 0109 loss_train: 0.2318 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7245s\n",
            "Epoch: 0110 loss_train: 0.2318 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7282s\n",
            "Epoch: 0111 loss_train: 0.2317 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7248s\n",
            "Epoch: 0112 loss_train: 0.2317 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7260s\n",
            "Epoch: 0113 loss_train: 0.2315 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7244s\n",
            "Epoch: 0114 loss_train: 0.2313 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7243s\n",
            "Epoch: 0115 loss_train: 0.2312 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7234s\n",
            "Epoch: 0116 loss_train: 0.2312 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7233s\n",
            "Epoch: 0117 loss_train: 0.2311 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7238s\n",
            "Epoch: 0118 loss_train: 0.2312 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7245s\n",
            "Epoch: 0119 loss_train: 0.2309 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7257s\n",
            "Epoch: 0120 loss_train: 0.2308 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7351s\n",
            "Epoch: 0121 loss_train: 0.2308 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7261s\n",
            "Epoch: 0122 loss_train: 0.2305 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7324s\n",
            "Epoch: 0123 loss_train: 0.2304 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7267s\n",
            "Epoch: 0124 loss_train: 0.2302 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7246s\n",
            "Epoch: 0125 loss_train: 0.2300 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7250s\n",
            "Epoch: 0126 loss_train: 0.2301 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7251s\n",
            "Epoch: 0127 loss_train: 0.2299 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7283s\n",
            "Epoch: 0128 loss_train: 0.2299 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7274s\n",
            "Epoch: 0129 loss_train: 0.2296 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7280s\n",
            "Epoch: 0130 loss_train: 0.2298 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7301s\n",
            "Epoch: 0131 loss_train: 0.2295 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7277s\n",
            "Epoch: 0132 loss_train: 0.2294 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7278s\n",
            "Epoch: 0133 loss_train: 0.2292 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7253s\n",
            "Epoch: 0134 loss_train: 0.2291 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7262s\n",
            "Epoch: 0135 loss_train: 0.2289 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7255s\n",
            "Epoch: 0136 loss_train: 0.2289 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7263s\n",
            "Epoch: 0137 loss_train: 0.2287 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7260s\n",
            "Epoch: 0138 loss_train: 0.2287 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7274s\n",
            "Epoch: 0139 loss_train: 0.2284 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7266s\n",
            "Epoch: 0140 loss_train: 0.2282 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7281s\n",
            "Epoch: 0141 loss_train: 0.2281 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7264s\n",
            "Epoch: 0142 loss_train: 0.2281 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7262s\n",
            "Epoch: 0143 loss_train: 0.2280 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7266s\n",
            "Epoch: 0144 loss_train: 0.2279 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7266s\n",
            "Epoch: 0145 loss_train: 0.2275 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7274s\n",
            "Epoch: 0146 loss_train: 0.2276 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7307s\n",
            "Epoch: 0147 loss_train: 0.2276 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7296s\n",
            "Epoch: 0148 loss_train: 0.2272 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7292s\n",
            "Epoch: 0149 loss_train: 0.2271 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7291s\n",
            "Epoch: 0150 loss_train: 0.2267 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7290s\n",
            "Epoch: 0151 loss_train: 0.2267 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7280s\n",
            "Epoch: 0152 loss_train: 0.2267 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7275s\n",
            "Epoch: 0153 loss_train: 0.2264 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7277s\n",
            "Epoch: 0154 loss_train: 0.2263 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7274s\n",
            "Epoch: 0155 loss_train: 0.2260 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7266s\n",
            "Epoch: 0156 loss_train: 0.2262 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7286s\n",
            "Epoch: 0157 loss_train: 0.2259 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7283s\n",
            "Epoch: 0158 loss_train: 0.2255 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7275s\n",
            "Epoch: 0159 loss_train: 0.2254 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7295s\n",
            "Epoch: 0160 loss_train: 0.2251 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7283s\n",
            "Epoch: 0161 loss_train: 0.2250 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7274s\n",
            "Epoch: 0162 loss_train: 0.2247 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7282s\n",
            "Epoch: 0163 loss_train: 0.2245 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7291s\n",
            "Epoch: 0164 loss_train: 0.2244 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7279s\n",
            "Epoch: 0165 loss_train: 0.2240 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7315s\n",
            "Epoch: 0166 loss_train: 0.2241 acc_train: 0.9389 loss_val: 0.2414 acc_val: 0.9350 time: 0.7291s\n",
            "Epoch: 0167 loss_train: 0.2238 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7302s\n",
            "Epoch: 0168 loss_train: 0.2236 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7304s\n",
            "Epoch: 0169 loss_train: 0.2234 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7297s\n",
            "Epoch: 0170 loss_train: 0.2235 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7289s\n",
            "Epoch: 0171 loss_train: 0.2230 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7278s\n",
            "Epoch: 0172 loss_train: 0.2231 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7273s\n",
            "Epoch: 0173 loss_train: 0.2225 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7277s\n",
            "Epoch: 0174 loss_train: 0.2227 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7292s\n",
            "Epoch: 0175 loss_train: 0.2219 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7317s\n",
            "Epoch: 0176 loss_train: 0.2218 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7310s\n",
            "Epoch: 0177 loss_train: 0.2218 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7384s\n",
            "Epoch: 0178 loss_train: 0.2216 acc_train: 0.9389 loss_val: 0.2415 acc_val: 0.9350 time: 0.7471s\n",
            "Epoch: 0179 loss_train: 0.2214 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7626s\n",
            "Epoch: 0180 loss_train: 0.2212 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7271s\n",
            "Epoch: 0181 loss_train: 0.2208 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7282s\n",
            "Epoch: 0182 loss_train: 0.2203 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7270s\n",
            "Epoch: 0183 loss_train: 0.2206 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7264s\n",
            "Epoch: 0184 loss_train: 0.2201 acc_train: 0.9389 loss_val: 0.2416 acc_val: 0.9350 time: 0.7298s\n",
            "Epoch: 0185 loss_train: 0.2198 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7331s\n",
            "Epoch: 0186 loss_train: 0.2194 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7589s\n",
            "Epoch: 0187 loss_train: 0.2192 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7715s\n",
            "Epoch: 0188 loss_train: 0.2189 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7580s\n",
            "Epoch: 0189 loss_train: 0.2187 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7815s\n",
            "Epoch: 0190 loss_train: 0.2180 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7443s\n",
            "Epoch: 0191 loss_train: 0.2181 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7321s\n",
            "Epoch: 0192 loss_train: 0.2178 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7275s\n",
            "Epoch: 0193 loss_train: 0.2178 acc_train: 0.9389 loss_val: 0.2417 acc_val: 0.9350 time: 0.7260s\n",
            "Epoch: 0194 loss_train: 0.2172 acc_train: 0.9389 loss_val: 0.2418 acc_val: 0.9350 time: 0.7261s\n",
            "Epoch: 0195 loss_train: 0.2171 acc_train: 0.9389 loss_val: 0.2418 acc_val: 0.9350 time: 0.7258s\n",
            "Epoch: 0196 loss_train: 0.2162 acc_train: 0.9389 loss_val: 0.2418 acc_val: 0.9350 time: 0.7252s\n",
            "Epoch: 0197 loss_train: 0.2164 acc_train: 0.9389 loss_val: 0.2418 acc_val: 0.9350 time: 0.7249s\n",
            "Epoch: 0198 loss_train: 0.2159 acc_train: 0.9389 loss_val: 0.2419 acc_val: 0.9350 time: 0.7270s\n",
            "Epoch: 0199 loss_train: 0.2157 acc_train: 0.9389 loss_val: 0.2419 acc_val: 0.9350 time: 0.7256s\n",
            "Epoch: 0200 loss_train: 0.2157 acc_train: 0.9389 loss_val: 0.2419 acc_val: 0.9350 time: 0.7257s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: IndexError: index is out of bounds for dimension with size 0\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "output = model(features, adj)\n",
        "\n",
        "# Assuming labels is a tensor on the correct device\n",
        "loss_test = criterion(output[idx_test], labels_tensor[idx_test])\n",
        "acc_test = cal_accuracy(output[idx_test], labels_tensor[idx_test])\n",
        "\n",
        "print(\"Test set results:\",\n",
        "      \"loss= {:.4f}\".format(loss_test.item()),\n",
        "      \"accuracy= {:.4f}\".format(acc_test))\n"
      ],
      "metadata": {
        "id": "3vXolcfg59dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2차원 라벨로 예측"
      ],
      "metadata": {
        "id": "_QZ5_mHa9tlG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFzIZyLaFgE-"
      },
      "outputs": [],
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "\n",
        "# # Assuming you have predictions and true labels for your test set\n",
        "# # Replace these with your actual predictions and true labels\n",
        "# y_pred = torch.argmax(model(features, adj)[idx_test], -1).cpu().numpy()\n",
        "\n",
        "# # Fix: Ensure idx_test values are within the range of labels\n",
        "# # The original idx_test_valid was empty, causing y_true to be empty\n",
        "# # We need to adjust idx_test to be within the bounds of labels\n",
        "# idx_test_valid = [i - (train_size + vocab_length) for i in idx_test if i < len(labels) + (train_size + vocab_length)]\n",
        "# # idx_test is shifted by (train_size + vocab_length), we need to shift it back\n",
        "\n",
        "# # Extract true labels using the valid indices\n",
        "# y_true = [labels[i] for i in idx_test_valid]\n",
        "\n",
        "# # Now y_true and y_pred should have consistent lengths\n",
        "\n",
        "# cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "# disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "# disp.plot()\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1차원 라벨로 예측"
      ],
      "metadata": {
        "id": "hG9pcinU9nSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming you have predictions and true labels for your test set\n",
        "# Replace these with your actual predictions and true labels\n",
        "y_pred = torch.argmax(model(features, adj)[idx_test], -1).cpu().numpy()\n",
        "\n",
        "# Fix: Ensure idx_test values are within the range of labels\n",
        "# The original idx_test_valid was empty, causing y_true to be empty\n",
        "# We need to adjust idx_test to be within the bounds of labels\n",
        "idx_test_valid = [i - (train_size + vocab_length) for i in idx_test if i < len(labels) + (train_size + vocab_length)]\n",
        "# idx_test is shifted by (train_size + vocab_length), we need to shift it back\n",
        "\n",
        "# Extract true labels using the valid indices\n",
        "# Move the labels to CPU before converting to a list\n",
        "y_true = [labels[i].cpu().item() for i in idx_test_valid]\n",
        "\n",
        "# Now y_true and y_pred should have consistent lengths and be on the CPU\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Predicted 0', 'Predicted 1'], yticklabels=['Actual 0', 'Actual 1'])\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.title(f'{datasets[2]} with Top-n labels')\n",
        "plt.savefig('els_confusion_matrix.pdf')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UgYdkpJW8s4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDKzqC41R4F8"
      },
      "source": [
        "# Train and Validation results visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Juh2GOzADBX"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Plotting the metrics\n",
        "epochs = list(range(1, len(loss_train) + 1))\n",
        "\n",
        "dataset = 'CS'\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot for Loss\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, acc_train, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val, 'r', label='Validation accuracy')\n",
        "plt.plot(epochs, loss_train, 'b', label='Training loss')\n",
        "plt.plot(epochs, loss_val, 'r', label='Validation loss')\n",
        "plt.title(datasets[2] + '_Training and Validation')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy and Loss')\n",
        "plt.ylim(0, 1.0)\n",
        "plt.xlim(0, 200)\n",
        "plt.legend()\n",
        "\n",
        "# Plot for Accuracy\n",
        "# plt.subplot(1, 2, 2)\n",
        "\n",
        "# plt.title('Training and Validation Accuracy')\n",
        "# plt.xlabel('Epochs')\n",
        "# plt.ylabel('Accuracy')\n",
        "# plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQwlWq6dyYJm"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmPNukmk40gd"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    predictions = torch.argmax(output[idx_test],-1).cpu().tolist()\n",
        "    # Move test_labels to CPU and convert to NumPy array\n",
        "    test_labels_np = test_labels.cpu().numpy()\n",
        "    acc = accuracy_score(test_labels_np, predictions)  # Use the NumPy array\n",
        "    f11 = f1_score(test_labels_np,predictions, average='macro') # Use the NumPy array\n",
        "    f12 = f1_score(test_labels_np,predictions, average = 'micro') # Use the NumPy array\n",
        "    f13 = f1_score(test_labels_np,predictions, average = 'weighted') # Use the NumPy array\n",
        "    return acc, f11, f12, f13\n",
        "\n",
        "#print(test())\n",
        "# Call the test function and print the results\n",
        "acc, f1_macro, f1_micro, f1_weighted = test()\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"F1 Score (Macro):\", f1_macro)\n",
        "print(\"F1 Score (Micro):\", f1_micro)\n",
        "print(\"F1 Score (Weighted):\", f1_weighted)\n",
        "print(\"std:\", np.std(np.array(val_loss)))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: from sklearn.metrics import f1_score, accuracy_score\n",
        "# import numpy as np\n",
        "# def test():\n",
        "#     model.eval()\n",
        "#     output = model(features, adj)\n",
        "#     # Get predictions for all labels for each sample\n",
        "#     predictions = output[idx_test].cpu().detach().numpy()\n",
        "#     # test_labels is already a NumPy array; no need to call .cpu() or .numpy()\n",
        "#     test_labels_np = test_labels\n",
        "#     # Adjust idx_test to get valid indices for test labels\n",
        "#     idx_test_valid = [i - (train_size + vocab_length) for i in idx_test if i < len(labels) + (train_size + vocab_length)]\n",
        "#     # Extract the relevant predictions using valid indices\n",
        "#     predictions = predictions[:len(idx_test_valid)]\n",
        "#     # Assuming you want to predict the presence/absence of each label\n",
        "#     predictions = (predictions > 0.5).astype(int)\n",
        "#     # Calculate metrics for each label separately and then average\n",
        "#     # Reshape predictions to match test_labels_np shape\n",
        "#     # test_labels_np is (2000, 15) and predictions should be (2000, 15)\n",
        "#     # predictions = predictions.reshape(test_labels_np.shape)  # Remove or adjust this line\n",
        "#     # Instead of reshaping, extract predictions for the relevant labels\n",
        "#     predictions = predictions[:, :test_labels_np.shape[1]]  # Select predictions for the correct number of labels\n",
        "#     acc = accuracy_score(test_labels_np, predictions)\n",
        "#     f11 = f1_score(test_labels_np, predictions, average='macro')\n",
        "#     f12 = f1_score(test_labels_np, predictions, average='micro')\n",
        "#     f13 = f1_score(test_labels_np, predictions, average='weighted')\n",
        "#     return acc, f11, f12, f13\n",
        "# #print(test())\n",
        "# # Call the test function and print the results\n",
        "# acc, f1_macro, f1_micro, f1_weighted = test()\n",
        "# print(\"Accuracy:\", acc)\n",
        "# print(\"F1 Score (Macro):\", f1_macro)\n",
        "# print(\"F1 Score (Micro):\", f1_micro)\n",
        "# add std for acc anf f1\n",
        "\n",
        "print(\"F1 Score (Weighted):\", f1_weighted)\n",
        "\n",
        "# Calculate the standard deviation for accuracy and F1 scores over multiple runs\n",
        "# This requires running the training and testing process multiple times.\n",
        "# Here's an example of how you would do it, assuming you store results from multiple runs.\n",
        "\n",
        "num_runs = 5  # Number of times to run the training and testing process\n",
        "acc_scores = []\n",
        "f1_macro_scores = []\n",
        "f1_micro_scores = []\n",
        "f1_weighted_scores = []\n",
        "\n",
        "print(f\"\\nRunning evaluation {num_runs} times to calculate standard deviation...\")\n",
        "\n",
        "for run in range(num_runs):\n",
        "    print(f\"Run {run + 1}/{num_runs}\")\n",
        "    # Re-initialize model and optimizer for each run to get different results\n",
        "    model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT, n_layers=NUM_LAYERS).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    # Train the model (you can choose whether to show results for each run or not)\n",
        "    train_model(show_result=False)\n",
        "\n",
        "    # Evaluate the model\n",
        "    acc, f1_macro, f1_micro, f1_weighted = test()\n",
        "\n",
        "    # Store the results\n",
        "    acc_scores.append(acc)\n",
        "    f1_macro_scores.append(f1_macro)\n",
        "    f1_micro_scores.append(f1_micro)\n",
        "    f1_weighted_scores.append(f1_weighted)\n",
        "\n",
        "# Calculate the mean and standard deviation of the metrics\n",
        "mean_acc = np.mean(acc_scores)\n",
        "std_acc = np.std(acc_scores)\n",
        "\n",
        "mean_f1_macro = np.mean(f1_macro_scores)\n",
        "std_f1_macro = np.std(f1_macro_scores)\n",
        "\n",
        "mean_f1_micro = np.mean(f1_micro_scores)\n",
        "std_f1_micro = np.std(f1_micro_scores)\n",
        "\n",
        "mean_f1_weighted = np.mean(f1_weighted_scores)\n",
        "std_f1_weighted = np.std(f1_weighted_scores)\n",
        "\n",
        "print(\"\\nEvaluation Results (Mean ± Std Dev over {} runs):\".format(num_runs))\n",
        "print(\"Accuracy: {:.4f} ± {:.4f}\".format(mean_acc, std_acc))\n",
        "print(\"F1 Score (Macro): {:.4f} ± {:.4f}\".format(mean_f1_macro, std_f1_macro))\n",
        "print(\"F1 Score (Micro): {:.4f} ± {:.4f}\".format(mean_f1_micro, std_f1_micro))\n",
        "print(\"F1 Score (Weighted): {:.4f} ± {:.4f}\".format(mean_f1_weighted, std_f1_weighted))\n"
      ],
      "metadata": {
        "id": "0AQQTnvwBJt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: RuntimeError: \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Int'\n",
        "\n",
        "# The error \"nll_loss_forward_reduce_cuda_kernel_2d_index\" not implemented for 'Int'\n",
        "# typically occurs when the target tensor (labels) for `nn.CrossEntropyLoss`\n",
        "# is of type `torch.IntTensor` when it should be `torch.LongTensor` for CUDA\n",
        "# operations.\n",
        "\n",
        "# Looking at your code, the `labels` tensor is created as `torch.LongTensor`.\n",
        "# However, there might be a place where a subset of `labels` is being accessed\n",
        "# and potentially converted to `IntTensor` before being passed to the criterion.\n",
        "\n",
        "# Let's ensure that `labels_tensor` in the training loop is explicitly\n",
        "# cast to `torch.long` before being used with the criterion, even though\n",
        "# it's initially created as `LongTensor`. This adds an extra layer of\n",
        "# safety to prevent unexpected type conversions.\n",
        "\n",
        "# Also, in the `test` function, when converting `test_labels` to a NumPy array,\n",
        "# ensure `test_labels` is moved to the CPU first if it's on the GPU.\n",
        "\n",
        "# Let's modify the `train_model` and `test` functions.\n",
        "\n",
        "def train_model(show_result = True):\n",
        "    val_loss = []\n",
        "    train_loss = [] # List to store training losses\n",
        "    train_acc = []  # List to store training accuracies\n",
        "    val_acc = []    # List to store validation accuracies\n",
        "\n",
        "    # Ensure labels are on the correct device and type is long\n",
        "    labels_tensor = labels.to(device).long()\n",
        "\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        t = time.time()\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        output = model(features, adj)\n",
        "\n",
        "        # Ensure labels_tensor slice is also long\n",
        "        loss_train_epoch = criterion(output[idx_train], labels_tensor[idx_train].long())\n",
        "        acc_train_epoch = cal_accuracy(output[idx_train], labels_tensor[idx_train])\n",
        "        loss_train_epoch.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Store training loss and accuracy for this epoch\n",
        "        train_loss.append(loss_train_epoch.item())\n",
        "        train_acc.append(acc_train_epoch)\n",
        "\n",
        "        model.eval()\n",
        "        output = model(features, adj)\n",
        "\n",
        "        # Ensure labels_tensor slice is also long\n",
        "        loss_val_epoch = criterion(output[idx_val], labels_tensor[idx_val].long())\n",
        "        val_loss.append(loss_val_epoch.item())\n",
        "        acc_val_epoch = cal_accuracy(output[idx_val], labels_tensor[idx_val])\n",
        "\n",
        "        # Store validation accuracy for this epoch\n",
        "        val_acc.append(acc_val_epoch)\n",
        "\n",
        "        if show_result:\n",
        "            print(  'Epoch: {:04d}'.format(epoch+1),\n",
        "                    'loss_train: {:.4f}'.format(loss_train_epoch.item()),\n",
        "                    'acc_train: {:.4f}'.format(acc_train_epoch),\n",
        "                    'loss_val: {:.4f}'.format(loss_val_epoch.item()),\n",
        "                    'acc_val: {:.4f}'.format(acc_val_epoch),\n",
        "                    'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "        if epoch > EARLY_STOPPING and np.min(val_loss[-EARLY_STOPPING:]) > np.min(val_loss[:-EARLY_STOPPING]) :\n",
        "            if show_result:\n",
        "                print(\"Early Stopping...\")\n",
        "            break\n",
        "\n",
        "    return train_loss, train_acc, val_loss, val_acc # Return the collected metrics\n",
        "\n",
        "def test():\n",
        "    model.eval()\n",
        "    output = model(features, adj)\n",
        "    predictions = torch.argmax(output[idx_test],-1).cpu().tolist()\n",
        "\n",
        "    # Move test_labels to CPU before converting to NumPy array and ensure it's long\n",
        "    test_labels_np = labels[idx_test].cpu().long().numpy()\n",
        "    acc = accuracy_score(test_labels_np, predictions)\n",
        "    f11 = f1_score(test_labels_np, predictions, average='macro')\n",
        "    f12 = f1_score(test_labels_np, predictions, average = 'micro')\n",
        "    f13 = f1_score(test_labels_np, predictions, average = 'weighted')\n",
        "    return acc, f11, f12, f13\n",
        "\n",
        "# Call train_model and store the returned metrics\n",
        "loss_train, acc_train, loss_val, acc_val = train_model()\n",
        "\n",
        "# Test the model\n",
        "model.eval()\n",
        "output = model(features, adj)\n",
        "\n",
        "# Ensure labels for testing are on the correct device and type is long\n",
        "labels_tensor_test = labels[idx_test].to(device).long()\n",
        "loss_test = criterion(output[idx_test], labels_tensor_test)\n",
        "acc_test = cal_accuracy(output[idx_test], labels_tensor_test)\n",
        "\n",
        "\n",
        "print(\"Test set results:\",\n",
        "      \"loss= {:.4f}\".format(loss_test.item()),\n",
        "      \"accuracy= {:.4f}\".format(acc_test))\n",
        "\n",
        "# Assuming you have predictions and true labels for your test set\n",
        "# Replace these with your actual predictions and true labels\n",
        "y_pred = torch.argmax(model(features, adj)[idx_test], -1).cpu().numpy()\n",
        "\n",
        "# Extract true labels using the valid indices and move to CPU as long\n",
        "y_true = labels[idx_test].cpu().long().numpy()\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues') # Removed xticklabels and yticklabels for generality\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "# You might want to dynamically generate labels based on num_class\n",
        "# plt.title(f'{datasets[2]} with Top-n labels') # datasets[2] might not be defined or correct\n",
        "plt.title('Confusion Matrix')\n",
        "# plt.savefig('els_confusion_matrix.pdf') # Consider a more general filename or make it dynamic\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plotting the metrics\n",
        "epochs = list(range(1, len(loss_train) + 1))\n",
        "\n",
        "# dataset = 'CS' # This variable is not consistently defined\n",
        "plt.figure(figsize=(14, 7))\n",
        "\n",
        "# Plot for Loss and Accuracy\n",
        "plt.subplot(1, 1, 1) # Using a single subplot for both loss and accuracy\n",
        "plt.plot(epochs, acc_train, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, acc_val, 'r', label='Validation accuracy')\n",
        "plt.plot(epochs, loss_train, 'b--', label='Training loss') # Use different linestyle for loss\n",
        "plt.plot(epochs, loss_val, 'r--', label='Validation loss') # Use different linestyle for loss\n",
        "plt.title('Training and Validation Metrics') # More general title\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Metrics Value') # General label for combined plot\n",
        "plt.ylim(0, max(max(acc_train), max(acc_val), max(loss_train), max(loss_val)) * 1.1) # Adjust ylim based on data\n",
        "plt.xlim(0, NUM_EPOCHS)\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Call the test function and print the results\n",
        "acc, f1_macro, f1_micro, f1_weighted = test()\n",
        "print(\"Accuracy:\", acc)\n",
        "print(\"F1 Score (Macro):\", f1_macro)\n",
        "print(\"F1 Score (Micro):\", f1_micro)\n",
        "print(\"F1 Score (Weighted):\", f1_weighted)\n",
        "\n",
        "\n",
        "# Calculate the standard deviation for accuracy and F1 scores over multiple runs\n",
        "# This requires running the training and testing process multiple times.\n",
        "# Here's an example of how you would do it, assuming you store results from multiple runs.\n",
        "\n",
        "num_runs = 5  # Number of times to run the training and testing process\n",
        "acc_scores = []\n",
        "f1_macro_scores = []\n",
        "f1_micro_scores = []\n",
        "f1_weighted_scores = []\n",
        "\n",
        "print(f\"\\nRunning evaluation {num_runs} times to calculate standard deviation...\")\n",
        "\n",
        "for run in range(num_runs):\n",
        "    print(f\"Run {run + 1}/{num_runs}\")\n",
        "    # Re-initialize model and optimizer for each run to get different results\n",
        "    model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT, n_layers=NUM_LAYERS).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "    # Train the model (you can choose whether to show results for each run or not)\n",
        "    train_model(show_result=False)\n",
        "\n",
        "    # Evaluate the model\n",
        "    acc, f1_macro, f1_micro, f1_weighted = test()\n",
        "\n",
        "    # Store the results\n",
        "    acc_scores.append(acc)\n",
        "    f1_macro_scores.append(f1_macro)\n",
        "    f1_micro_scores.append(f1_micro)\n",
        "    f1_weighted_scores.append(f1_weighted)\n",
        "\n",
        "# Calculate the mean and standard deviation of the metrics\n",
        "mean_acc = np.mean(acc_scores)\n",
        "std_acc = np.std(acc_scores)\n",
        "\n",
        "mean_f1_macro = np.mean(f1_macro_scores)\n",
        "std_f1_macro = np.std(f1_macro_scores)\n",
        "\n",
        "mean_f1_micro = np.mean(f1_micro_scores)\n",
        "std_f1_micro = np.std(f1_micro_scores)\n",
        "\n",
        "mean_f1_weighted = np.mean(f1_weighted_scores)\n",
        "std_f1_weighted = np.std(f1_weighted_scores)\n",
        "\n",
        "print(\"\\nEvaluation Results (Mean ± Std Dev over {} runs):\".format(num_runs))\n",
        "print(\"Accuracy: {:.4f} ± {:.4f}\".format(mean_acc, std_acc))\n",
        "print(\"F1 Score (Macro): {:.4f} ± {:.4f}\".format(mean_f1_macro, std_f1_macro))\n",
        "print(\"F1 Score (Micro): {:.4f} ± {:.4f}\".format(mean_f1_micro, std_f1_micro))\n",
        "print(\"F1 Score (Weighted): {:.4f} ± {:.4f}\".format(mean_f1_weighted, std_f1_weighted))\n",
        "\n"
      ],
      "metadata": {
        "id": "8cPBSoq7Ccs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV4xX563JSZt"
      },
      "source": [
        "# Coverage Error Rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-YnbNW88JM34"
      },
      "outputs": [],
      "source": [
        "# prompt: write about coverage error\n",
        "\n",
        "def coverage_error_rate(predictions, true_labels):\n",
        "    \"\"\"\n",
        "    Calculate coverage error rate.\n",
        "    Args:\n",
        "        predictions: A list of model predictions.\n",
        "        true_labels: A list of true labels.\n",
        "    Returns:\n",
        "        The coverage error rate.\n",
        "    \"\"\"\n",
        "    if not predictions or not true_labels:\n",
        "        return 0.0  # Handle empty lists\n",
        "\n",
        "    num_examples = len(predictions)\n",
        "    coverage_errors = 0\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        if predictions[i] != true_labels[i]:\n",
        "            coverage_errors += 1\n",
        "\n",
        "    coverage_error_rate = coverage_errors / num_examples\n",
        "    return coverage_error_rate\n",
        "\n",
        "\n",
        "# Example Usage (Assuming you have predictions and true labels from your model)\n",
        "# predictions = model.predict(X_test)  # Replace with your model's prediction method\n",
        "# Call the model with the input data to get predictions\n",
        "output = model(features, adj)  # Assuming 'features' and 'adj' are your input data\n",
        "predictions = torch.argmax(output[idx_test], -1).cpu().tolist()  # Get predicted labels\n",
        "\n",
        "# Assuming 'labels' contains your ground truth labels and idx_test contains the indices for the test set\n",
        "true_labels = [labels[i] for i in idx_test] # Use list comprehension to create the true labels using idx_test\n",
        "\n",
        "coverage_error = coverage_error_rate(predictions, true_labels)\n",
        "print(\"Coverage Error Rate:\", coverage_error)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgMjcefSWMOp"
      },
      "outputs": [],
      "source": [
        "# prompt: write about coverage error\n",
        "\n",
        "def coverage_error_rate(predictions, true_labels):\n",
        "    \"\"\"\n",
        "    Calculate coverage error rate.\n",
        "    Args:\n",
        "        predictions: A list of model predictions.\n",
        "        true_labels: A list of true labels.\n",
        "    Returns:\n",
        "        The coverage error rate.\n",
        "    \"\"\"\n",
        "    if not predictions or not true_labels:\n",
        "        return 0.0  # Handle empty lists\n",
        "\n",
        "    num_examples = len(predictions)\n",
        "    coverage_errors = 0\n",
        "\n",
        "    for i in range(num_examples):\n",
        "        if predictions[i] != true_labels[i]:\n",
        "            coverage_errors += 1\n",
        "\n",
        "    coverage_error_rate = coverage_errors / num_examples\n",
        "    return coverage_error_rate\n",
        "\n",
        "\n",
        "# Example Usage (Assuming you have predictions and true labels from your model)\n",
        "# predictions = model.predict(X_test)  # Replace with your model's prediction method\n",
        "# Call the model with the input data to get predictions\n",
        "output = model(features, adj)  # Assuming 'features' and 'adj' are your input data\n",
        "predictions = torch.argmax(output[idx_test], -1).cpu().tolist()  # Get predicted labels\n",
        "\n",
        "# Assuming 'labels' contains your ground truth labels and idx_test contains the indices for the test set\n",
        "# Fix: Ensure idx_test indices are within the bounds of labels\n",
        "true_labels = [labels[i] for i in idx_test if i < len(labels)] # Use list comprehension with a condition\n",
        "\n",
        "coverage_error = coverage_error_rate(predictions, true_labels)\n",
        "print(\"Coverage Error Rate:\", coverage_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWL6gpP2d_yE"
      },
      "source": [
        "# Evaluation results visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kUiaHzsWAfCp"
      },
      "outputs": [],
      "source": [
        "# Bar plot code\n",
        "\n",
        "metrics = ['Accuracy', 'F1 Macro', 'F1 Micro', 'F1 Weighted']\n",
        "scores = [acc, f1_macro, f1_micro, f1_weighted]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(metrics, scores, color=['blue', 'green', 'red', 'purple'])\n",
        "plt.ylim(0, 1)\n",
        "plt.xlabel('Metrics')\n",
        "plt.ylabel('F1-Scores')\n",
        "plt.title('Model Performance Metrics (CS_dataset)')\n",
        "for i in range(len(scores)):\n",
        "    plt.text(i, scores[i] + 0.01, f'{scores[i]:.2f}', ha='center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exSbal15HNyf"
      },
      "source": [
        "# Write the input results in csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PdApH73XHMsI"
      },
      "outputs": [],
      "source": [
        "# import scipy.sparse as sp\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# import torch\n",
        "\n",
        "# # Assuming adj is the symmetric adjacency matrix from your code snippet as a PyTorch Tensor\n",
        "\n",
        "# # Convert the PyTorch sparse tensor to a dense tensor\n",
        "# adj_dense_tensor = adj.to_dense()\n",
        "\n",
        "# # Convert the dense PyTorch Tensor to a NumPy array\n",
        "# adj_numpy = adj_dense_tensor.numpy()\n",
        "\n",
        "# # Convert the NumPy array to a scipy sparse matrix (if necessary)\n",
        "# adj_sparse = sp.csr_matrix(adj_numpy)  # Or another sparse format if appropriate\n",
        "\n",
        "# # Convert the sparse matrix to a dense matrix\n",
        "# adj_dense = adj_sparse.todense()\n",
        "\n",
        "# # Convert the dense matrix to a DataFrame\n",
        "# adj_df = pd.DataFrame(adj_dense)\n",
        "\n",
        "# # Write the DataFrame to a CSV file\n",
        "# adj_df.to_csv('CS_adjacency_matrix.csv', index=True, header=True)\n",
        "\n",
        "# print(\"Adjacency matrix has been written to 'CS_adjacency_matrix.csv'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8JJ__UMHWRT"
      },
      "source": [
        "# Predict the output data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jRgFEcR-Je91"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "\n",
        "# def predict_documents(model, features, adj, cs_unique_words, idx_test):\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         output = model(features, adj)\n",
        "#         predictions = torch.argmax(output[idx_test], 1).cpu().tolist()\n",
        "#     return predictions\n",
        "\n",
        "# # Assuming cs_unique_words contains the actual documents for better clarity\n",
        "# documents = sentences  # Replace with actual documents\n",
        "# #Indices of test documents, replace with your actual test indices\n",
        "\n",
        "# # Get the predictions\n",
        "# predictions = predict_documents(model, features, adj, cs_unique_words, idx_test)\n",
        "\n",
        "# # Print the predictions along with the corresponding documents\n",
        "# for i, prediction in enumerate(predictions):\n",
        "#     print(f\" Predicted Label: {prediction}, Document: {documents[i]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BINuDCTeC0W7"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "\n",
        "# def predict_documents(model, features, adj, idx_test):\n",
        "#     model.eval()\n",
        "#     with torch.no_grad():\n",
        "#         output = model(features, adj)\n",
        "#         predictions = torch.argmax(output[idx_test], 1).cpu().tolist()\n",
        "#     return predictions\n",
        "\n",
        "# # Assuming cs_unique_words contains the actual documents for better clarity\n",
        "# documents = sentences  # Replace with actual documents\n",
        "\n",
        "# # Indices of test documents, replace with your actual test indices\n",
        "# #idx_test = [0, 1, 2, 3, 4]  # Replace with your actual test indices\n",
        "\n",
        "# # Assuming label_to_word_map is a dictionary that maps labels to words\n",
        "# label_to_word_map = {0:'network', 1:'detection', 2:'image', 3:'system', 4:'computer'}  # Replace with actual mapping\n",
        "\n",
        "# # Get the predictions\n",
        "# predictions = predict_documents(model, features, adj, idx_test)\n",
        "\n",
        "# # Print the predictions along with the corresponding documents\n",
        "# for i, prediction in enumerate(predictions):\n",
        "#     predicted_word = label_to_word_map[prediction]  # Convert label to word\n",
        "#     print(f\"Predicted Label: {predicted_word}, Document: {documents[i]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7msCGEn1NqC6"
      },
      "outputs": [],
      "source": [
        "# # Perform a forward pass to get predictions\n",
        "# output = model(features, adj)\n",
        "\n",
        "# # Assuming the word embedding is the first entry in the features tensor\n",
        "# word_pred = output[1]\n",
        "\n",
        "# # Get the predicted document\n",
        "# _, predicted_doc = word_pred.max(dim=-1)\n",
        "\n",
        "# print(f\"Predicted document for the word '{cs_unique_words}': {predicted_doc.item()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ-pWfCOCS8h"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LOFsVlv4hTgc"
      },
      "source": [
        "# Test 10 times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TtdlFpUlz2-"
      },
      "outputs": [],
      "source": [
        "# test_acc_list = []\n",
        "# test_f11_list = []\n",
        "# test_f12_list = []\n",
        "# test_f13_list = []\n",
        "\n",
        "# for t in range(10):\n",
        "#     model = GCN(nfeat=node_size, nhid=HIDDEN_DIM, nclass=num_class, dropout=DROP_OUT,n_layers=NUM_LAYERS).to(device)\n",
        "#     optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
        "#     idx_train, idx_val, idx_test = generate_train_val()\n",
        "#     train_model(show_result=False)\n",
        "#     # unpack all four return values from the test() function\n",
        "#     acc, f11, f12, f13 = test() # assign the unused fourth return value to _\n",
        "#     test_acc_list.append(acc)\n",
        "#     test_f11_list.append(f11)\n",
        "#     test_f12_list.append(f12)\n",
        "#     test_f13_list.append(f13)\n",
        "\n",
        "\n",
        "# print(\"Accuracy:\",np.round(np.mean(test_acc_list),4))\n",
        "# print(\"Macro F1:\",np.round(np.mean(test_f11_list),4))\n",
        "# print(\"Micro F1:\",np.round(np.mean(test_f12_list),4))\n",
        "# print(\"Weighted F1:\",np.round(np.mean(test_f13_list),4))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7aa5370929264d4d9b49873dc790c2de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_31b6ba592bfe4ff3bc0d93ed57e66909",
              "IPY_MODEL_4fe1ba2fb3b743e49d1bdc21da4a0b49",
              "IPY_MODEL_145c11efe2b24cacb9d630b782561fd4"
            ],
            "layout": "IPY_MODEL_a87e348731394ef884abf4051437cb68"
          }
        },
        "31b6ba592bfe4ff3bc0d93ed57e66909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_932edc72a8ea416994ee5fb6d58f0132",
            "placeholder": "​",
            "style": "IPY_MODEL_a3a14471e6334865b2c4ca821d19652c",
            "value": "100%"
          }
        },
        "4fe1ba2fb3b743e49d1bdc21da4a0b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_292da0fcd01c4fadb8758924716c50db",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b43c97188d054b4e95ac1bd871eff696",
            "value": 10000
          }
        },
        "145c11efe2b24cacb9d630b782561fd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5981e8d3acc248cd8627e6146ce985ae",
            "placeholder": "​",
            "style": "IPY_MODEL_8f4254b16af344229ff79b4716896996",
            "value": " 10000/10000 [01:16&lt;00:00, 152.92it/s]"
          }
        },
        "a87e348731394ef884abf4051437cb68": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "932edc72a8ea416994ee5fb6d58f0132": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3a14471e6334865b2c4ca821d19652c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "292da0fcd01c4fadb8758924716c50db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b43c97188d054b4e95ac1bd871eff696": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5981e8d3acc248cd8627e6146ce985ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f4254b16af344229ff79b4716896996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ace831efc0b4e8baaaab32c5d9d5a82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5bab41534674409f97fd08b145d27737",
              "IPY_MODEL_013aab60b32c4e67bee2b1a072ee32a9",
              "IPY_MODEL_7d3ce23fe96742338a9b69170b36e512"
            ],
            "layout": "IPY_MODEL_6e6e0e1241ed443ebb3d7fd85845e2db"
          }
        },
        "5bab41534674409f97fd08b145d27737": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_227f23e470b84cd2be605344b9f10aeb",
            "placeholder": "​",
            "style": "IPY_MODEL_9b95353a0bf147b89ae86d377e027fc9",
            "value": "100%"
          }
        },
        "013aab60b32c4e67bee2b1a072ee32a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_960cf9d192c44c6c94822a468ee8469a",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce777699e3e24ba79457541d1605c772",
            "value": 10000
          }
        },
        "7d3ce23fe96742338a9b69170b36e512": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c99da0d989174bb5a14c35d7eb9972ad",
            "placeholder": "​",
            "style": "IPY_MODEL_0ea9f931f704478c97eb0620706c3df0",
            "value": " 10000/10000 [01:16&lt;00:00, 94.39it/s]"
          }
        },
        "6e6e0e1241ed443ebb3d7fd85845e2db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "227f23e470b84cd2be605344b9f10aeb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b95353a0bf147b89ae86d377e027fc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "960cf9d192c44c6c94822a468ee8469a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce777699e3e24ba79457541d1605c772": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c99da0d989174bb5a14c35d7eb9972ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ea9f931f704478c97eb0620706c3df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8ed3e5daab54cb59d188047ce073374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4200c73135e5450bb3b3595722f08e10",
              "IPY_MODEL_479327a06815455c9127e029d84a9151",
              "IPY_MODEL_66b12391239c4b628faccf485da9de6f"
            ],
            "layout": "IPY_MODEL_62a9f6a3aeb942eeab2c4d319010ea0d"
          }
        },
        "4200c73135e5450bb3b3595722f08e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb427ae58a454e368ac23ce3dfc49278",
            "placeholder": "​",
            "style": "IPY_MODEL_79a2b2aba27149a4a12094775bf82338",
            "value": "100%"
          }
        },
        "479327a06815455c9127e029d84a9151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cee0f61cdb3b4798bad47aaff64100ba",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_63e4aa9cd5954d1eb903f3fad890c868",
            "value": 10000
          }
        },
        "66b12391239c4b628faccf485da9de6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4879f1adae214efc9afb4a81f1c6f31c",
            "placeholder": "​",
            "style": "IPY_MODEL_754a4bd6e8914e0cad33cdd59fffa0e8",
            "value": " 10000/10000 [13:17&lt;00:00, 471.87it/s]"
          }
        },
        "62a9f6a3aeb942eeab2c4d319010ea0d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb427ae58a454e368ac23ce3dfc49278": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79a2b2aba27149a4a12094775bf82338": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cee0f61cdb3b4798bad47aaff64100ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63e4aa9cd5954d1eb903f3fad890c868": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4879f1adae214efc9afb4a81f1c6f31c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "754a4bd6e8914e0cad33cdd59fffa0e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "566040d57cfc43f7be16b31c30a321aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72b17c729658444fadb3432e82fa28fd",
              "IPY_MODEL_41ad5ba280f242388842ae0bf9e63401",
              "IPY_MODEL_e6be34bdfbcf4588b5221cefeac56324"
            ],
            "layout": "IPY_MODEL_32618fb9dfea4f2da30fe7db9493a290"
          }
        },
        "72b17c729658444fadb3432e82fa28fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d3cc2b7534c461aa303c08ca9c55a0f",
            "placeholder": "​",
            "style": "IPY_MODEL_521ed81c3e6242879d51dca501a6060f",
            "value": "100%"
          }
        },
        "41ad5ba280f242388842ae0bf9e63401": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b841ddcdeb224dfc801b8a070382e13c",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e0d449aed844076b14fff3a4f209c0b",
            "value": 10000
          }
        },
        "e6be34bdfbcf4588b5221cefeac56324": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7415b0b02f7547ef8fd2529035b155ab",
            "placeholder": "​",
            "style": "IPY_MODEL_8d5fb2a40a4242e48218c5be7ed8800a",
            "value": " 10000/10000 [12:58&lt;00:00, 469.94it/s]"
          }
        },
        "32618fb9dfea4f2da30fe7db9493a290": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d3cc2b7534c461aa303c08ca9c55a0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "521ed81c3e6242879d51dca501a6060f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b841ddcdeb224dfc801b8a070382e13c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e0d449aed844076b14fff3a4f209c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7415b0b02f7547ef8fd2529035b155ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d5fb2a40a4242e48218c5be7ed8800a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}